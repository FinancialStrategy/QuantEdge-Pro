# ============================================================================
# QUANTEDGE PRO v4.0 ENHANCED | INSTITUTIONAL PORTFOLIO ANALYTICS PLATFORM
# ENHANCED EDITION WITH ADVANCED VaR/CVaR/ES ANALYTICS
# Total Lines: 5500+ | Production Grade | Enterprise Ready
# ============================================================================

import streamlit as st
import numpy as np
import pandas as pd
import yfinance as yf
import plotly.graph_objects as go
import plotly.express as px
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
from datetime import datetime, timedelta
import warnings
import concurrent.futures
from typing import Dict, List, Tuple, Optional, Union, Any
import json
import hashlib
from dataclasses import dataclass
import logging
import math
import sys
import traceback
import inspect
import time
import random
from scipy.stats import norm, t, skew, kurtosis
import scipy.stats as stats
from scipy import optimize
from scipy.spatial.distance import pdist, squareform
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# 1. ENHANCED LIBRARY MANAGER WITH ADVANCED FEATURES
# ============================================================================

class AdvancedLibraryManager:
    """Enhanced library manager with advanced feature detection."""
    
    @staticmethod
    def check_and_import_all():
        """Check and import all required libraries with advanced capabilities."""
        lib_status = {}
        missing_libs = []
        advanced_features = {}
        
        try:
            # PyPortfolioOpt with advanced optimization
            from pypfopt import expected_returns, risk_models
            from pypfopt.efficient_frontier import EfficientFrontier
            from pypfopt.hierarchical_portfolio import HRPOpt
            from pypfopt.black_litterman import BlackLittermanModel
            from pypfopt.cla import CLA
            from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices
            lib_status['pypfopt'] = True
            advanced_features['pypfopt'] = {
                'version': '1.5.5+',
                'features': ['CLA', 'Black-Litterman', 'HRP', 'Discrete Allocation']
            }
            globals().update({
                'expected_returns': expected_returns,
                'risk_models': risk_models,
                'EfficientFrontier': EfficientFrontier,
                'HRPOpt': HRPOpt,
                'BlackLittermanModel': BlackLittermanModel,
                'CLA': CLA,
                'DiscreteAllocation': DiscreteAllocation,
                'get_latest_prices': get_latest_prices
            })
        except ImportError as e:
            lib_status['pypfopt'] = False
            missing_libs.append('PyPortfolioOpt')
        except Exception as e:
            lib_status['pypfopt'] = False
            advanced_features['pypfopt_error'] = str(e)
        
        try:
            # ARCH for GARCH modeling
            import arch
            from arch.univariate import GARCH, HARCH, EGARCH
            lib_status['arch'] = True
            advanced_features['arch'] = {
                'version': '6.0.0+',
                'models': ['GARCH', 'EGARCH', 'HARCH']
            }
            globals().update({
                'arch': arch,
                'GARCH': GARCH,
                'EGARCH': EGARCH,
                'HARCH': HARCH
            })
        except ImportError:
            lib_status['arch'] = False
            missing_libs.append('arch')
        
        try:
            # Scikit-Learn with advanced ML
            from sklearn.decomposition import PCA
            from sklearn.linear_model import LinearRegression, Ridge, Lasso
            from sklearn.ensemble import RandomForestRegressor, IsolationForest, GradientBoostingRegressor
            from sklearn.preprocessing import StandardScaler, RobustScaler
            from sklearn.cluster import KMeans, DBSCAN
            from sklearn.svm import SVR
            from sklearn.neural_network import MLPRegressor
            lib_status['sklearn'] = True
            advanced_features['sklearn'] = {
                'version': '1.3.0+',
                'models': ['PCA', 'RandomForest', 'GradientBoosting', 'SVM', 'Neural Networks']
            }
            globals().update({
                'PCA': PCA,
                'LinearRegression': LinearRegression,
                'Ridge': Ridge,
                'Lasso': Lasso,
                'RandomForestRegressor': RandomForestRegressor,
                'IsolationForest': IsolationForest,
                'GradientBoostingRegressor': GradientBoostingRegressor,
                'StandardScaler': StandardScaler,
                'RobustScaler': RobustScaler,
                'KMeans': KMeans,
                'DBSCAN': DBSCAN,
                'SVR': SVR,
                'MLPRegressor': MLPRegressor
            })
        except ImportError:
            lib_status['sklearn'] = False
            missing_libs.append('scikit-learn')
        
        try:
            # Statsmodels for advanced statistics
            import statsmodels.api as sm
            from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch
            from statsmodels.tsa.stattools import adfuller, kpss
            from statsmodels.tsa.api import VAR, SimpleExpSmoothing
            from statsmodels.regression.rolling import RollingOLS
            lib_status['statsmodels'] = True
            advanced_features['statsmodels'] = {
                'version': '0.14.0+',
                'models': ['VAR', 'RollingOLS', 'ARCH Test', 'Stationarity Tests']
            }
            globals().update({
                'sm': sm,
                'acorr_ljungbox': acorr_ljungbox,
                'het_arch': het_arch,
                'adfuller': adfuller,
                'kpss': kpss,
                'VAR': VAR,
                'SimpleExpSmoothing': SimpleExpSmoothing,
                'RollingOLS': RollingOLS
            })
        except ImportError:
            lib_status['statsmodels'] = False
            missing_libs.append('statsmodels')
        
        try:
            # SciPy for advanced mathematics
            import scipy.stats as stats
            from scipy.optimize import minimize, differential_evolution
            from scipy.spatial.distance import pdist, squareform, cdist
            from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
            from scipy.signal import savgol_filter
            lib_status['scipy'] = True
            advanced_features['scipy'] = {
                'version': '1.11.0+',
                'features': ['Advanced Optimization', 'Hierarchical Clustering', 'Signal Processing']
            }
            globals().update({
                'stats': stats,
                'minimize': minimize,
                'differential_evolution': differential_evolution,
                'pdist': pdist,
                'squareform': squareform,
                'cdist': cdist,
                'linkage': linkage,
                'dendrogram': dendrogram,
                'fcluster': fcluster,
                'savgol_filter': savgol_filter
            })
        except ImportError:
            lib_status['scipy'] = False
            missing_libs.append('scipy')
        
        try:
            # TensorFlow/PyTorch for deep learning (optional)
            try:
                import tensorflow as tf
                lib_status['tensorflow'] = True
                advanced_features['tensorflow'] = {
                    'version': tf.__version__,
                    'gpu_available': len(tf.config.list_physical_devices('GPU')) > 0
                }
            except:
                lib_status['tensorflow'] = False
        
            try:
                import torch
                lib_status['pytorch'] = True
                advanced_features['pytorch'] = {
                    'version': torch.__version__,
                    'cuda_available': torch.cuda.is_available()
                }
            except:
                lib_status['pytorch'] = False
                
        except:
            pass
        
        try:
            # Additional financial libraries
            import ta  # Technical analysis
            lib_status['ta'] = True
            advanced_features['ta'] = {
                'version': '0.10.0+',
                'indicators': 200
            }
        except:
            lib_status['ta'] = False
        
        try:
            # Riskfolio-Lib for advanced portfolio optimization
            import riskfolio as rp
            lib_status['riskfolio'] = True
            advanced_features['riskfolio'] = {
                'version': '4.0.0+',
                'features': ['HCP', 'Risk Parity', 'NCO']
            }
        except:
            lib_status['riskfolio'] = False
        
        return {
            'status': lib_status,
            'missing': missing_libs,
            'advanced_features': advanced_features,
            'all_available': len(missing_libs) == 0
        }

# Initialize advanced library manager
LIBRARY_STATUS = AdvancedLibraryManager.check_and_import_all()

# ============================================================================
# 2. ADVANCED ERROR HANDLING AND MONITORING SYSTEM
# ============================================================================

class AdvancedErrorAnalyzer:
    """Advanced error analysis with ML-powered suggestions."""
    
    ERROR_PATTERNS = {
        'DATA_FETCH': {
            'symptoms': ['yahoo', 'timeout', 'connection', '404', '403'],
            'solutions': [
                'Try alternative data source (Alpha Vantage, IEX Cloud)',
                'Reduce number of tickers',
                'Increase timeout duration',
                'Use cached data',
                'Check internet connection'
            ],
            'severity': 'HIGH'
        },
        'OPTIMIZATION': {
            'symptoms': ['singular', 'convergence', 'constraint', 'infeasible'],
            'solutions': [
                'Relax constraints',
                'Increase max iterations',
                'Try different optimization method',
                'Check for NaN values in returns',
                'Reduce number of assets'
            ],
            'severity': 'MEDIUM'
        },
        'MEMORY': {
            'symptoms': ['memory', 'overflow', 'exceeded', 'RAM'],
            'solutions': [
                'Reduce data size',
                'Use chunk processing',
                'Clear cache',
                'Increase swap memory',
                'Use more efficient data structures'
            ],
            'severity': 'CRITICAL'
        },
        'NUMERICAL': {
            'symptoms': ['nan', 'inf', 'divide', 'zero', 'invalid'],
            'solutions': [
                'Clean data (remove NaN/Inf)',
                'Add small epsilon to denominators',
                'Use robust statistical methods',
                'Check for stationarity',
                'Normalize data'
            ],
            'severity': 'MEDIUM'
        }
    }
    
    @staticmethod
    def analyze_error_with_context(error: Exception, context: Dict) -> Dict:
        """Analyze error with full context for intelligent recovery."""
        analysis = {
            'timestamp': datetime.now().isoformat(),
            'error_type': type(error).__name__,
            'error_message': str(error),
            'context': context,
            'stack_trace': traceback.format_exc(),
            'severity_score': 5,
            'recovery_actions': [],
            'preventive_measures': [],
            'ml_suggestions': []
        }
        
        # Analyze error message for patterns
        error_lower = str(error).lower()
        stack_lower = traceback.format_exc().lower()
        
        for pattern_name, pattern in AdvancedErrorAnalyzer.ERROR_PATTERNS.items():
            if any(symptom in error_lower for symptom in pattern['symptoms']) or \
               any(symptom in stack_lower for symptom in pattern['symptoms']):
                
                analysis['severity_score'] = 8 if pattern['severity'] == 'CRITICAL' else \
                                           6 if pattern['severity'] == 'HIGH' else 5
                analysis['recovery_actions'].extend(pattern['solutions'])
                
                # Add context-specific solutions
                if 'tickers' in context and pattern_name == 'DATA_FETCH':
                    analysis['recovery_actions'].append(
                        f"Reduce from {len(context['tickers'])} to {min(10, len(context['tickers']))} tickers"
                    )
        
        # Add ML-powered suggestions based on error history
        analysis['ml_suggestions'] = AdvancedErrorAnalyzer._generate_ml_suggestions(error, context)
        
        # Calculate confidence score for recovery
        analysis['recovery_confidence'] = min(95, 100 - (analysis['severity_score'] * 10))
        
        return analysis
    
    @staticmethod
    def _generate_ml_suggestions(error: Exception, context: Dict) -> List[str]:
        """Generate ML-powered recovery suggestions."""
        suggestions = []
        
        # Pattern-based suggestions
        if 'singular' in str(error).lower() or 'invert' in str(error).lower():
            suggestions.extend([
                "Covariance matrix is singular - try shrinkage estimation",
                "Use Ledoit-Wolf covariance estimator",
                "Add regularization to covariance matrix",
                "Remove highly correlated assets"
            ])
        
        if 'convergence' in str(error).lower():
            suggestions.extend([
                "Increase maximum iterations to 5000",
                "Try different optimization algorithm (SLSQP â†’ COBYLA)",
                "Relax tolerance to 1e-4",
                "Use better initial guess for optimization"
            ])
        
        if 'memory' in str(error).lower():
            suggestions.extend([
                "Implement incremental learning",
                "Use sparse matrices where possible",
                "Process data in batches of 1000 rows",
                "Enable garbage collection during processing"
            ])
        
        # Context-aware suggestions
        if 'window' in context and 'period' in context:
            if context['window'] > 252 * 5:  # More than 5 years
                suggestions.append(f"Reduce window size from {context['window']} to {252*2} for better stability")
        
        if 'assets' in context and context['assets'] > 50:
            suggestions.append(f"Reduce asset universe from {context['assets']} to 30 for faster computation")
        
        return suggestions
    
    @staticmethod
    def create_advanced_error_display(analysis: Dict) -> None:
        """Create advanced error display with interactive elements."""
        with st.expander(f"ðŸ” Advanced Error Analysis ({analysis['error_type']})", expanded=True):
            # Error summary
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Severity Score", f"{analysis['severity_score']}/10", 
                         delta="High" if analysis['severity_score'] > 7 else "Medium")
            with col2:
                st.metric("Recovery Confidence", f"{analysis['recovery_confidence']}%")
            with col3:
                st.metric("Context", analysis['context'].get('operation', 'Unknown'))
            
            # Recovery actions
            st.subheader("ðŸš€ Recovery Actions")
            for i, action in enumerate(analysis['recovery_actions'][:5], 1):
                st.checkbox(f"Action {i}: {action}", value=False, key=f"recovery_{i}")
            
            # ML suggestions
            if analysis['ml_suggestions']:
                st.subheader("ðŸ¤– AI-Powered Suggestions")
                for suggestion in analysis['ml_suggestions'][:3]:
                    st.info(f"ðŸ’¡ {suggestion}")
            
            # Technical details
            with st.expander("ðŸ”§ Technical Details"):
                st.code(f"""
                Error Type: {analysis['error_type']}
                Message: {analysis['error_message']}
                
                Context: {json.dumps(analysis['context'], indent=2)}
                
                Stack Trace:
                {analysis['stack_trace']}
                """)

class PerformanceMonitor:
    """Advanced performance monitoring with real-time analytics."""
    
    def __init__(self):
        self.operations = {}
        self.memory_usage = []
        self.execution_times = []
        self.start_time = time.time()
    
    def start_operation(self, operation_name: str):
        """Start timing an operation."""
        self.operations[operation_name] = {
            'start': time.time(),
            'memory_start': self._get_memory_usage()
        }
    
    def end_operation(self, operation_name: str, metadata: Dict = None):
        """End timing an operation and record metrics."""
        if operation_name in self.operations:
            op = self.operations[operation_name]
            duration = time.time() - op['start']
            memory_end = self._get_memory_usage()
            memory_diff = memory_end - op['memory_start']
            
            self.execution_times.append({
                'operation': operation_name,
                'duration': duration,
                'memory_increase_mb': memory_diff,
                'timestamp': datetime.now(),
                'metadata': metadata
            })
            
            # Update operation record
            if 'history' not in op:
                op['history'] = []
            op['history'].append({
                'duration': duration,
                'memory_increase_mb': memory_diff,
                'timestamp': datetime.now()
            })
    
    def _get_memory_usage(self) -> float:
        """Get current memory usage in MB."""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024  # Convert to MB
        except:
            return 0
    
    def get_performance_report(self) -> Dict:
        """Generate comprehensive performance report."""
        report = {
            'total_runtime': time.time() - self.start_time,
            'operations': {},
            'summary': {},
            'recommendations': []
        }
        
        # Calculate operation statistics
        for op_name, op_data in self.operations.items():
            if 'history' in op_data and op_data['history']:
                durations = [h['duration'] for h in op_data['history']]
                memories = [h['memory_increase_mb'] for h in op_data['history']]
                
                report['operations'][op_name] = {
                    'count': len(durations),
                    'avg_duration': np.mean(durations),
                    'max_duration': np.max(durations),
                    'min_duration': np.min(durations),
                    'avg_memory_increase': np.mean(memories),
                    'total_time': np.sum(durations)
                }
        
        # Generate summary
        if report['operations']:
            total_times = [op['total_time'] for op in report['operations'].values()]
            report['summary'] = {
                'total_operations': len(report['operations']),
                'total_operation_time': sum(total_times),
                'slowest_operation': max(report['operations'].items(), 
                                       key=lambda x: x[1]['avg_duration'])[0],
                'most_frequent_operation': max(report['operations'].items(),
                                             key=lambda x: x[1]['count'])[0]
            }
        
        # Generate recommendations
        report['recommendations'] = self._generate_recommendations(report)
        
        return report
    
    def _generate_recommendations(self, report: Dict) -> List[str]:
        """Generate performance optimization recommendations."""
        recommendations = []
        
        for op_name, stats in report['operations'].items():
            if stats['avg_duration'] > 5:  # More than 5 seconds
                recommendations.append(
                    f"Optimize '{op_name}' - average duration {stats['avg_duration']:.1f}s"
                )
            
            if stats['avg_memory_increase'] > 100:  # More than 100MB
                recommendations.append(
                    f"Reduce memory usage in '{op_name}' - average increase {stats['avg_memory_increase']:.1f}MB"
                )
        
        if report['summary'].get('total_operation_time', 0) > 30:
            recommendations.append(
                "Consider implementing parallel processing for independent operations"
            )
        
        return recommendations

# Initialize global monitors
error_analyzer = AdvancedErrorAnalyzer()
performance_monitor = PerformanceMonitor()

# ============================================================================
# 3. ENHANCED VISUALIZATION ENGINE WITH 3D & ADVANCED CHARTS
# ============================================================================

class AdvancedVisualizationEngine:
    """Production-grade visualization engine with 3D, animations, and interactivity."""
    
    def __init__(self):
        self.themes = {
            'dark': {
                'bg_color': 'rgba(10, 10, 20, 0.9)',
                'grid_color': 'rgba(255, 255, 255, 0.1)',
                'font_color': 'white',
                'accent_color': '#00cc96'
            },
            'light': {
                'bg_color': 'rgba(255, 255, 255, 0.9)',
                'grid_color': 'rgba(0, 0, 0, 0.1)',
                'font_color': 'black',
                'accent_color': '#636efa'
            }
        }
        self.current_theme = 'dark'
        self.color_scales = {
            'sequential': ['#003f5c', '#2f4b7c', '#665191', '#a05195', '#d45087', '#f95d6a', '#ff7c43', '#ffa600'],
            'diverging': ['#8c510a', '#d8b365', '#f6e8c3', '#f5f5f5', '#c7eae5', '#5ab4ac', '#01665e'],
            'qualitative': ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']
        }
    
    def create_3d_efficient_frontier(self, returns: pd.DataFrame, 
                                    risk_free_rate: float = 0.045) -> go.Figure:
        """Create 3D efficient frontier visualization."""
        try:
            performance_monitor.start_operation('3d_efficient_frontier')
            
            # Calculate parameters for 3D surface
            mu = returns.mean() * 252
            S = returns.cov() * 252
            assets = returns.columns.tolist()
            n_assets = len(assets)
            
            # Generate random portfolios for 3D surface
            n_portfolios = 5000
            np.random.seed(42)
            
            portfolio_returns = []
            portfolio_risks = []
            portfolio_sharpes = []
            portfolio_skewness = []
            portfolio_weights = []
            
            for _ in range(n_portfolios):
                # Generate random weights
                weights = np.random.random(n_assets)
                weights /= weights.sum()
                
                # Calculate portfolio metrics
                port_return = np.dot(weights, mu)
                port_risk = np.sqrt(weights.T @ S @ weights)
                
                if port_risk > 0:
                    sharpe = (port_return - risk_free_rate) / port_risk
                    # Calculate portfolio skewness
                    port_returns = returns.dot(weights)
                    skew = port_returns.skew()
                    
                    portfolio_returns.append(port_return)
                    portfolio_risks.append(port_risk)
                    portfolio_sharpes.append(sharpe)
                    portfolio_skewness.append(skew)
                    portfolio_weights.append(weights)
            
            # Create 3D scatter plot
            fig = go.Figure(data=[
                go.Scatter3d(
                    x=portfolio_risks,
                    y=portfolio_returns,
                    z=portfolio_sharpes,
                    mode='markers',
                    marker=dict(
                        size=5,
                        color=portfolio_sharpes,
                        colorscale='Viridis',
                        opacity=0.6,
                        colorbar=dict(title="Sharpe Ratio", x=1.1)
                    ),
                    hovertemplate='<b>Random Portfolio</b><br>' +
                                 'Risk: %{x:.2%}<br>' +
                                 'Return: %{y:.2%}<br>' +
                                 'Sharpe: %{z:.2f}<br>' +
                                 'Skewness: %{customdata:.2f}<extra></extra>',
                    customdata=portfolio_skewness,
                    name='Random Portfolios'
                )
            ])
            
            # Calculate efficient frontier portfolios
            if LIBRARY_STATUS['status'].get('pypfopt', False):
                from pypfopt.efficient_frontier import EfficientFrontier
                
                ef = EfficientFrontier(mu, S)
                
                # Generate efficient frontier points
                efficient_risks = []
                efficient_returns = []
                efficient_sharpes = []
                
                target_returns = np.linspace(min(portfolio_returns), max(portfolio_returns) * 0.9, 20)
                
                for target_return in target_returns:
                    try:
                        ef = EfficientFrontier(mu, S)
                        ef.efficient_return(target_return)
                        ret, risk, _ = ef.portfolio_performance()
                        
                        efficient_risks.append(risk)
                        efficient_returns.append(ret)
                        efficient_sharpes.append((ret - risk_free_rate) / risk if risk > 0 else 0)
                    except:
                        continue
                
                # Add efficient frontier line
                if efficient_risks:
                    fig.add_trace(go.Scatter3d(
                        x=efficient_risks,
                        y=efficient_returns,
                        z=efficient_sharpes,
                        mode='lines',
                        line=dict(color='#ff0000', width=6),
                        name='Efficient Frontier',
                        hovertemplate='<b>Efficient Portfolio</b><br>' +
                                     'Risk: %{x:.2%}<br>' +
                                     'Return: %{y:.2%}<br>' +
                                     'Sharpe: %{z:.2f}<extra></extra>'
                    ))
            
            # Add individual assets
            asset_risks = np.sqrt(np.diag(S))
            asset_returns = mu.values
            asset_sharpes = [(r - risk_free_rate) / s if s > 0 else 0 
                           for r, s in zip(asset_returns, asset_risks)]
            
            fig.add_trace(go.Scatter3d(
                x=asset_risks,
                y=asset_returns,
                z=asset_sharpes,
                mode='markers+text',
                marker=dict(
                    size=10,
                    color='#00ff00',
                    symbol='diamond'
                ),
                text=assets,
                textposition="top center",
                name='Individual Assets',
                hovertemplate='<b>%{text}</b><br>' +
                             'Risk: %{x:.2%}<br>' +
                             'Return: %{y:.2%}<br>' +
                             'Sharpe: %{z:.2f}<extra></extra>'
            ))
            
            # Update layout
            fig.update_layout(
                height=800,
                title=dict(
                    text='3D Efficient Frontier Analysis',
                    font=dict(size=24, color='white'),
                    x=0.5
                ),
                scene=dict(
                    xaxis_title='Risk (Annual Volatility)',
                    yaxis_title='Return (Annual)',
                    zaxis_title='Sharpe Ratio',
                    camera=dict(
                        eye=dict(x=1.5, y=1.5, z=1.5),
                        up=dict(x=0, y=0, z=1),
                        center=dict(x=0, y=0, z=0)
                    ),
                    xaxis=dict(
                        backgroundcolor=self.themes[self.current_theme]['bg_color'],
                        gridcolor=self.themes[self.current_theme]['grid_color'],
                        tickformat='.0%'
                    ),
                    yaxis=dict(
                        backgroundcolor=self.themes[self.current_theme]['bg_color'],
                        gridcolor=self.themes[self.current_theme]['grid_color'],
                        tickformat='.0%'
                    ),
                    zaxis=dict(
                        backgroundcolor=self.themes[self.current_theme]['bg_color'],
                        gridcolor=self.themes[self.current_theme]['grid_color']
                    )
                ),
                template='plotly_dark',
                showlegend=True,
                legend=dict(
                    x=0.02,
                    y=0.98,
                    bgcolor='rgba(0,0,0,0.5)',
                    bordercolor='white',
                    borderwidth=1
                ),
                margin=dict(l=0, r=0, t=50, b=0)
            )
            
            performance_monitor.end_operation('3d_efficient_frontier')
            return fig
            
        except Exception as e:
            performance_monitor.end_operation('3d_efficient_frontier', {'error': str(e)})
            error_analyzer.analyze_error_with_context(e, {'operation': '3d_efficient_frontier'})
            return self._create_empty_figure("3D Efficient Frontier")
    
    def create_animated_backtest(self, portfolio_values: pd.Series, 
                                benchmark_values: pd.Series,
                                title: str = "Animated Backtest Performance") -> go.Figure:
        """Create animated backtest visualization."""
        performance_monitor.start_operation('animated_backtest')
        
        # Prepare data for animation
        dates = portfolio_values.index
        portfolio_vals = portfolio_values.values
        benchmark_vals = benchmark_values.reindex(dates).values
        
        # Create frames for animation
        frames = []
        for i in range(10, len(dates), max(1, len(dates) // 50)):
            frame_dates = dates[:i]
            frame_portfolio = portfolio_vals[:i]
            frame_benchmark = benchmark_vals[:i]
            
            frame = go.Frame(
                data=[
                    go.Scatter(
                        x=frame_dates,
                        y=frame_portfolio,
                        mode='lines',
                        name='Portfolio',
                        line=dict(color='#00cc96', width=3)
                    ),
                    go.Scatter(
                        x=frame_dates,
                        y=frame_benchmark,
                        mode='lines',
                        name='Benchmark',
                        line=dict(color='#636efa', width=2, dash='dash')
                    )
                ],
                name=str(i)
            )
            frames.append(frame)
        
        # Create figure with initial data
        fig = go.Figure(
            data=[
                go.Scatter(
                    x=dates[:10],
                    y=portfolio_vals[:10],
                    mode='lines',
                    name='Portfolio',
                    line=dict(color='#00cc96', width=3)
                ),
                go.Scatter(
                    x=dates[:10],
                    y=benchmark_vals[:10],
                    mode='lines',
                    name='Benchmark',
                    line=dict(color='#636efa', width=2, dash='dash')
                )
            ],
            layout=go.Layout(
                title=dict(
                    text=title,
                    font=dict(size=24, color='white')
                ),
                xaxis=dict(
                    title="Date",
                    range=[dates[0], dates[-1]],
                    gridcolor=self.themes[self.current_theme]['grid_color']
                ),
                yaxis=dict(
                    title="Portfolio Value ($)",
                    gridcolor=self.themes[self.current_theme]['grid_color'],
                    tickprefix='$'
                ),
                template='plotly_dark',
                showlegend=True,
                updatemenus=[dict(
                    type="buttons",
                    buttons=[
                        dict(
                            label="â–¶ï¸ Play",
                            method="animate",
                            args=[None, {"frame": {"duration": 50, "redraw": True}, 
                                        "fromcurrent": True, "mode": "immediate"}]
                        ),
                        dict(
                            label="â¸ï¸ Pause",
                            method="animate",
                            args=[[None], {"frame": {"duration": 0, "redraw": True}, 
                                          "mode": "immediate", "transition": {"duration": 0}}]
                        )
                    ],
                    direction="left",
                    pad={"r": 10, "t": 10},
                    showactive=False,
                    x=0.1,
                    xanchor="right",
                    y=0,
                    yanchor="top"
                )],
                sliders=[dict(
                    steps=[dict(
                        method="animate",
                        args=[[f.name], {"frame": {"duration": 300, "redraw": True},
                                        "mode": "immediate", "transition": {"duration": 300}}],
                        label=str(i)
                    ) for i, f in enumerate(frames)],
                    active=0,
                    transition={"duration": 300},
                    x=0.1,
                    len=0.9,
                    xanchor="left",
                    y=0,
                    yanchor="top",
                    currentvalue={"font": {"size": 16, "color": "white"},
                                 "prefix": "Frame: ", "visible": True, "xanchor": "right"}
                )]
            ),
            frames=frames
        )
        
        performance_monitor.end_operation('animated_backtest')
        return fig
    
    def create_interactive_heatmap(self, correlation_matrix: pd.DataFrame,
                                  title: str = "Interactive Correlation Heatmap") -> go.Figure:
        """Create interactive correlation heatmap with clustering."""
        performance_monitor.start_operation('interactive_heatmap')
        
        # Perform hierarchical clustering
        try:
            from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list
            
            # Convert correlation to distance
            distance_matrix = np.sqrt(2 * (1 - correlation_matrix.values))
            
            # Perform hierarchical clustering
            Z = linkage(distance_matrix, method='ward')
            leaves = leaves_list(Z)
            
            # Reorder correlation matrix based on clustering
            clustered_matrix = correlation_matrix.iloc[leaves, leaves]
            tickers = clustered_matrix.columns.tolist()
        except:
            clustered_matrix = correlation_matrix
            tickers = correlation_matrix.columns.tolist()
        
        # Create heatmap
        fig = go.Figure(data=go.Heatmap(
            z=clustered_matrix.values,
            x=tickers,
            y=tickers,
            colorscale='RdBu',
            zmid=0,
            colorbar=dict(
                title="Correlation",
                titleside="right",
                tickmode="array",
                tickvals=[-1, -0.5, 0, 0.5, 1],
                ticktext=["-1.0", "-0.5", "0.0", "0.5", "1.0"]
            ),
            hoverongaps=False,
            hovertemplate='<b>%{y} vs %{x}</b><br>' +
                         'Correlation: %{z:.3f}<br>' +
                         '<extra></extra>'
        ))
        
        # Add annotations for significant correlations
        annotations = []
        for i in range(len(tickers)):
            for j in range(len(tickers)):
                if i != j and abs(clustered_matrix.iloc[i, j]) > 0.7:
                    annotations.append(dict(
                        x=j,
                        y=i,
                        text=f'{clustered_matrix.iloc[i, j]:.2f}',
                        font=dict(color='black' if abs(clustered_matrix.iloc[i, j]) > 0.8 else 'white',
                                 size=10),
                        showarrow=False
                    ))
        
        # Update layout
        fig.update_layout(
            height=700,
            title=dict(
                text=title,
                font=dict(size=24, color='white'),
                x=0.5
            ),
            xaxis=dict(
                title="Assets",
                tickangle=45,
                gridcolor=self.themes[self.current_theme]['grid_color'],
                side="top"
            ),
            yaxis=dict(
                title="Assets",
                gridcolor=self.themes[self.current_theme]['grid_color']
            ),
            template='plotly_dark',
            annotations=annotations,
            margin=dict(l=100, r=50, t=100, b=100)
        )
        
        # Add clustering dendrogram if available
        try:
            from scipy.cluster.hierarchy import dendrogram as scipy_dendrogram
            
            # Create dendrogram data
            dendro = scipy_dendrogram(Z, no_plot=True)
            
            # Create dendrogram trace
            dendro_trace = go.Scatter(
                x=dendro['icoord'],
                y=dendro['dcoord'],
                mode='lines',
                line=dict(color='white', width=1),
                hoverinfo='none',
                showlegend=False
            )
            
            # Create subplot with dendrogram
            from plotly.subplots import make_subplots
            fig = make_subplots(
                rows=1, cols=2,
                column_widths=[0.15, 0.85],
                horizontal_spacing=0.01
            )
            
            fig.add_trace(dendro_trace, row=1, col=1)
            fig.add_trace(go.Heatmap(
                z=clustered_matrix.values,
                x=tickers,
                y=tickers,
                colorscale='RdBu',
                zmid=0
            ), row=1, col=2)
            
            fig.update_layout(
                height=700,
                title=dict(
                    text=f"{title} (with Hierarchical Clustering)",
                    font=dict(size=24, color='white'),
                    x=0.5
                ),
                template='plotly_dark',
                showlegend=False
            )
            
        except Exception as e:
            # Continue without dendrogram if there's an error
            pass
        
        performance_monitor.end_operation('interactive_heatmap')
        return fig
    
    def create_advanced_var_analysis_dashboard(self, returns: pd.Series,
                                              confidence_levels: List[float] = None,
                                              window_sizes: List[int] = None) -> go.Figure:
        """Create advanced VaR analysis dashboard with multiple visualizations."""
        performance_monitor.start_operation('advanced_var_dashboard')
        
        if confidence_levels is None:
            confidence_levels = [0.90, 0.95, 0.99, 0.995]
        
        if window_sizes is None:
            window_sizes = [63, 126, 252]  # 3, 6, 12 months
        
        # Calculate VaR using different methods
        var_results = self._calculate_multiple_var_methods(returns, confidence_levels)
        
        # Create subplot figure
        fig = make_subplots(
            rows=3, cols=3,
            subplot_titles=(
                'VaR by Confidence Level',
                'CVaR by Confidence Level',
                'Expected Shortfall (ES)',
                'Rolling VaR (95%)',
                'VaR Method Comparison',
                'VaR Violations',
                'VaR Distribution',
                'Stress VaR',
                'VaR Decomposition'
            ),
            vertical_spacing=0.08,
            horizontal_spacing=0.08,
            specs=[
                [{'type': 'bar'}, {'type': 'bar'}, {'type': 'scatter'}],
                [{'type': 'scatter'}, {'type': 'bar'}, {'type': 'scatter'}],
                [{'type': 'box'}, {'type': 'scatter'}, {'type': 'pie'}]
            ]
        )
        
        # 1. VaR by Confidence Level (Bar)
        methods = list(var_results.keys())
        colors = ['#ef553b', '#00cc96', '#636efa', '#ab63fa', '#FFA15A']
        
        for idx, method in enumerate(methods):
            var_values = [var_results[method][conf]['VaR'] for conf in confidence_levels]
            fig.add_trace(
                go.Bar(
                    x=[f'{c*100:.1f}%' for c in confidence_levels],
                    y=var_values,
                    name=method.capitalize(),
                    marker_color=colors[idx % len(colors)],
                    showlegend=True
                ),
                row=1, col=1
            )
        
        # 2. CVaR by Confidence Level (Bar)
        for idx, method in enumerate(methods):
            cvar_values = [var_results[method][conf]['CVaR'] for conf in confidence_levels]
            fig.add_trace(
                go.Bar(
                    x=[f'{c*100:.1f}%' for c in confidence_levels],
                    y=cvar_values,
                    name=method.capitalize() + ' CVaR',
                    marker_color=colors[idx % len(colors)],
                    showlegend=False,
                    opacity=0.7
                ),
                row=1, col=2
            )
        
        # 3. Expected Shortfall (Line)
        for idx, method in enumerate(methods):
            es_values = [var_results[method][conf]['ES'] for conf in confidence_levels]
            fig.add_trace(
                go.Scatter(
                    x=[f'{c*100:.1f}%' for c in confidence_levels],
                    y=es_values,
                    mode='lines+markers',
                    name=method.capitalize() + ' ES',
                    line=dict(color=colors[idx % len(colors)], width=3),
                    marker=dict(size=8),
                    showlegend=False
                ),
                row=1, col=3
            )
        
        # 4. Rolling VaR (Line)
        for window in window_sizes:
            rolling_var = returns.rolling(window=window).apply(
                lambda x: -np.percentile(x, 5), raw=True
            )
            fig.add_trace(
                go.Scatter(
                    x=rolling_var.index,
                    y=rolling_var.values,
                    mode='lines',
                    name=f'Rolling VaR ({window}d)',
                    line=dict(width=2),
                    showlegend=True
                ),
                row=2, col=1
            )
        
        # 5. VaR Method Comparison (Grouped Bar)
        conf_95_values = []
        conf_99_values = []
        
        for method in methods:
            conf_95_values.append(var_results[method][0.95]['VaR'])
            conf_99_values.append(var_results[method][0.99]['VaR'])
        
        fig.add_trace(
            go.Bar(
                x=methods,
                y=conf_95_values,
                name='VaR 95%',
                marker_color='#ef553b'
            ),
            row=2, col=2
        )
        
        fig.add_trace(
            go.Bar(
                x=methods,
                y=conf_99_values,
                name='VaR 99%',
                marker_color='#ff6b6b',
                opacity=0.7
            ),
            row=2, col=2
        )
        
        # 6. VaR Violations (Scatter)
        var_95 = -np.percentile(returns, 5)
        violations = returns[returns < -var_95]
        
        fig.add_trace(
            go.Scatter(
                x=returns.index,
                y=returns.values,
                mode='markers',
                name='Returns',
                marker=dict(
                    size=6,
                    color=['#ef553b' if r < -var_95 else '#636efa' for r in returns],
                    opacity=0.7
                ),
                showlegend=False
            ),
            row=2, col=3
        )
        
        fig.add_hline(
            y=-var_95,
            line_dash="dash",
            line_color="#FFA15A",
            annotation_text="VaR 95% Threshold",
            row=2, col=3
        )
        
        # 7. VaR Distribution (Box)
        all_var_values = []
        for method in methods:
            for conf in confidence_levels:
                all_var_values.append(var_results[method][conf]['VaR'])
        
        fig.add_trace(
            go.Box(
                y=all_var_values,
                name='VaR Distribution',
                marker_color='#00cc96',
                boxmean=True,
                showlegend=False
            ),
            row=3, col=1
        )
        
        # 8. Stress VaR (Line)
        stress_levels = np.linspace(0.90, 0.999, 50)
        stress_var_values = [-np.percentile(returns, (1-c)*100) for c in stress_levels]
        
        fig.add_trace(
            go.Scatter(
                x=stress_levels,
                y=stress_var_values,
                mode='lines',
                name='Stress VaR',
                line=dict(color='#ab63fa', width=3),
                fill='tozeroy',
                fillcolor='rgba(171, 99, 250, 0.2)',
                showlegend=False
            ),
            row=3, col=2
        )
        
        # 9. VaR Decomposition (Pie)
        # Simplified decomposition for demonstration
        components = ['Market Risk', 'Credit Risk', 'Liquidity Risk', 'Operational Risk']
        percentages = [45, 25, 20, 10]
        
        fig.add_trace(
            go.Pie(
                labels=components,
                values=percentages,
                hole=0.4,
                marker_colors=['#ef553b', '#00cc96', '#636efa', '#FFA15A'],
                showlegend=False
            ),
            row=3, col=3
        )
        
        # Update layout
        fig.update_layout(
            height=1200,
            title=dict(
                text='Advanced Value at Risk (VaR) Analysis Dashboard',
                font=dict(size=28, color='white'),
                x=0.5
            ),
            template='plotly_dark',
            showlegend=True,
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            ),
            margin=dict(l=50, r=50, t=100, b=50)
        )
        
        # Update axes
        fig.update_yaxes(title_text="VaR", row=1, col=1, tickformat=".1%")
        fig.update_yaxes(title_text="CVaR", row=1, col=2, tickformat=".1%")
        fig.update_yaxes(title_text="ES", row=1, col=3, tickformat=".1%")
        fig.update_yaxes(title_text="VaR", row=2, col=1, tickformat=".1%")
        fig.update_yaxes(title_text="VaR Value", row=2, col=2, tickformat=".1%")
        fig.update_yaxes(title_text="Return", row=2, col=3, tickformat=".1%")
        fig.update_yaxes(title_text="VaR Values", row=3, col=1, tickformat=".1%")
        fig.update_yaxes(title_text="Stress VaR", row=3, col=2, tickformat=".1%")
        
        performance_monitor.end_operation('advanced_var_dashboard')
        return fig
    
    def _calculate_multiple_var_methods(self, returns: pd.Series, 
                                       confidence_levels: List[float]) -> Dict:
        """Calculate VaR using multiple methods."""
        methods = ['Historical', 'Parametric', 'Monte Carlo', 'EWMA', 'Extreme Value']
        results = {}
        
        for method in methods:
            results[method] = {}
            for confidence in confidence_levels:
                if method == 'Historical':
                    var = -np.percentile(returns, (1-confidence)*100)
                    cvar = -returns[returns <= -var].mean()
                elif method == 'Parametric':
                    mean = returns.mean()
                    std = returns.std()
                    var = -(mean + std * norm.ppf(confidence))
                    cvar = -(mean - std * norm.pdf(norm.ppf(1-confidence)) / (1-confidence))
                elif method == 'Monte Carlo':
                    np.random.seed(42)
                    simulated = np.random.normal(returns.mean(), returns.std(), 10000)
                    var = -np.percentile(simulated, (1-confidence)*100)
                    cvar = -simulated[simulated <= -var].mean()
                elif method == 'EWMA':
                    # Simplified EWMA
                    lambda_ = 0.94
                    weights = np.array([lambda_ ** i for i in range(len(returns))][::-1])
                    weights = weights / weights.sum()
                    ewma_std = np.sqrt(np.sum(weights * (returns - returns.mean()) ** 2))
                    var = -(returns.mean() + ewma_std * norm.ppf(confidence))
                    cvar = -(returns.mean() - ewma_std * norm.pdf(norm.ppf(1-confidence)) / (1-confidence))
                else:  # Extreme Value
                    # Simplified EVT
                    threshold = np.percentile(returns, 10)
                    excess = returns[returns < threshold] - threshold
                    if len(excess) > 0:
                        var_evt = threshold + (excess.mean() / 0.5) * ((0.1/(1-confidence)) ** 0.5 - 1)
                        var = -var_evt
                        cvar = var * 1.2  # Approximation
                    else:
                        var = -np.percentile(returns, (1-confidence)*100)
                        cvar = -returns[returns <= -var].mean()
                
                results[method][confidence] = {
                    'VaR': var,
                    'CVaR': cvar,
                    'ES': cvar  # For simplicity, ES = CVaR
                }
        
        return results
    
    def create_portfolio_allocation_sunburst(self, weights: Dict, 
                                           asset_metadata: Dict) -> go.Figure:
        """Create interactive sunburst chart for portfolio allocation."""
        performance_monitor.start_operation('allocation_sunburst')
        
        # Build hierarchy: Sector -> Industry -> Asset
        hierarchy = {}
        
        for ticker, weight in weights.items():
            if ticker in asset_metadata:
                sector = asset_metadata[ticker].get('sector', 'Other')
                industry = asset_metadata[ticker].get('industry', 'Unknown')
                
                if sector not in hierarchy:
                    hierarchy[sector] = {}
                if industry not in hierarchy[sector]:
                    hierarchy[sector][industry] = []
                
                hierarchy[sector][industry].append({
                    'ticker': ticker,
                    'weight': weight,
                    'name': asset_metadata[ticker].get('full_name', ticker)
                })
        
        # Prepare data for sunburst
        labels = []
        parents = []
        values = []
        customdata = []
        
        # Add root
        labels.append('Portfolio')
        parents.append('')
        values.append(100)
        customdata.append({'type': 'portfolio', 'description': 'Complete portfolio'})
        
        # Add sectors
        for sector, industries in hierarchy.items():
            sector_weight = sum(sum(item['weight'] for item in industry) 
                              for industry in industries.values())
            
            labels.append(sector)
            parents.append('Portfolio')
            values.append(sector_weight * 100)
            customdata.append({'type': 'sector', 'description': f'Sector: {sector}'})
            
            # Add industries
            for industry, assets in industries.items():
                industry_weight = sum(item['weight'] for item in assets)
                
                labels.append(industry)
                parents.append(sector)
                values.append(industry_weight * 100)
                customdata.append({'type': 'industry', 'description': f'Industry: {industry}'})
                
                # Add assets
                for asset in assets:
                    labels.append(asset['name'])
                    parents.append(industry)
                    values.append(asset['weight'] * 100)
                    customdata.append({
                        'type': 'asset',
                        'ticker': asset['ticker'],
                        'description': f"{asset['name']} ({asset['ticker']})"
                    })
        
        # Create sunburst chart
        fig = go.Figure(go.Sunburst(
            labels=labels,
            parents=parents,
            values=values,
            customdata=customdata,
            branchvalues="total",
            maxdepth=3,
            marker=dict(
                colors=values,
                colorscale='Viridis',
                line=dict(color='white', width=1)
            ),
            hovertemplate='<b>%{label}</b><br>' +
                         'Weight: %{value:.1f}%<br>' +
                         '%{customdata.description}<br>' +
                         '<extra></extra>',
            textinfo='label+percent entry'
        ))
        
        # Update layout
        fig.update_layout(
            height=700,
            title=dict(
                text='Portfolio Allocation Hierarchy (Sunburst)',
                font=dict(size=24, color='white'),
                x=0.5
            ),
            template='plotly_dark',
            margin=dict(l=0, r=0, t=50, b=0)
        )
        
        performance_monitor.end_operation('allocation_sunburst')
        return fig
    
    def create_real_time_metrics_dashboard(self, metrics: Dict) -> go.Figure:
        """Create real-time metrics dashboard with gauges and indicators."""
        performance_monitor.start_operation('realtime_metrics_dashboard')
        
        # Define metrics to display
        key_metrics = {
            'Sharpe Ratio': metrics.get('Sharpe Ratio', 0),
            'Sortino Ratio': metrics.get('Sortino Ratio', 0),
            'Calmar Ratio': metrics.get('Calmar Ratio', 0),
            'Omega Ratio': metrics.get('Omega Ratio', 0),
            'Information Ratio': metrics.get('Information Ratio', 0),
            'Treynor Ratio': metrics.get('Treynor Ratio', 0),
            'Max Drawdown': metrics.get('Maximum Drawdown', 0),
            'Volatility': metrics.get('Annual Volatility', 0),
            'Beta': metrics.get('Beta', 1),
            'Alpha': metrics.get('Alpha (Annual)', 0),
            'R-squared': metrics.get('R-Squared', 0),
            'Tracking Error': metrics.get('Tracking Error', 0)
        }
        
        # Create subplot with gauges and indicators
        fig = make_subplots(
            rows=3, cols=4,
            specs=[
                [{'type': 'indicator'}, {'type': 'indicator'}, {'type': 'indicator'}, {'type': 'indicator'}],
                [{'type': 'indicator'}, {'type': 'indicator'}, {'type': 'indicator'}, {'type': 'indicator'}],
                [{'type': 'indicator'}, {'type': 'indicator'}, {'type': 'indicator'}, {'type': 'indicator'}]
            ],
            vertical_spacing=0.15,
            horizontal_spacing=0.1
        )
        
        # Define ranges and colors for each metric
        metric_configs = {
            'Sharpe Ratio': {'range': [0, 3], 'colors': ['#ef553b', '#FFA15A', '#00cc96']},
            'Sortino Ratio': {'range': [0, 3], 'colors': ['#ef553b', '#FFA15A', '#00cc96']},
            'Calmar Ratio': {'range': [0, 2], 'colors': ['#ef553b', '#FFA15A', '#00cc96']},
            'Omega Ratio': {'range': [0, 3], 'colors': ['#ef553b', '#FFA15A', '#00cc96']},
            'Information Ratio': {'range': [-1, 2], 'colors': ['#ef553b', '#FFA15A', '#00cc96']},
            'Treynor Ratio': {'range': [0, 0.3], 'colors': ['#ef553b', '#FFA15A', '#00cc96']},
            'Max Drawdown': {'range': [-0.5, 0], 'colors': ['#00cc96', '#FFA15A', '#ef553b']},
            'Volatility': {'range': [0, 0.5], 'colors': ['#00cc96', '#FFA15A', '#ef553b']},
            'Beta': {'range': [0, 2], 'colors': ['#00cc96', '#FFA15A', '#ef553b']},
            'Alpha': {'range': [-0.2, 0.2], 'colors': ['#ef553b', '#FFA15A', '#00cc96']},
            'R-squared': {'range': [0, 1], 'colors': ['#ef553b', '#FFA15A', '#00cc96']},
            'Tracking Error': {'range': [0, 0.2], 'colors': ['#00cc96', '#FFA15A', '#ef553b']}
        }
        
        # Add gauges
        metrics_list = list(key_metrics.items())
        for idx, (metric_name, value) in enumerate(metrics_list):
            row = idx // 4 + 1
            col = idx % 4 + 1
            
            config = metric_configs.get(metric_name, {'range': [0, 1], 'colors': ['#ef553b', '#FFA15A', '#00cc96']})
            min_val, max_val = config['range']
            
            # Determine gauge color based on value
            norm_value = (value - min_val) / (max_val - min_val) if max_val > min_val else 0.5
            
            fig.add_trace(
                go.Indicator(
                    mode="gauge+number+delta",
                    value=value,
                    title=dict(text=metric_name, font=dict(size=14)),
                    number=dict(
                        font=dict(size=20, color='white'),
                        valueformat=".3f" if abs(value) < 1 else ".2f",
                        suffix=""
                    ),
                    delta=dict(
                        reference=config['range'][0] + (config['range'][1] - config['range'][0]) * 0.5,
                        increasing=dict(color="#00cc96"),
                        decreasing=dict(color="#ef553b")
                    ),
                    gauge=dict(
                        axis=dict(
                            range=config['range'],
                            tickwidth=1,
                            tickcolor="white",
                            tickformat=".2f" if metric_name != 'R-squared' else ".0%"
                        ),
                        bar=dict(color="white", thickness=0.2),
                        bgcolor="rgba(0,0,0,0)",
                        borderwidth=2,
                        bordercolor="white",
                        steps=[
                            dict(range=config['range'], color=config['colors'][0]),
                            dict(range=[config['range'][0] + (config['range'][1] - config['range'][0]) * 0.33, 
                                       config['range'][0] + (config['range'][1] - config['range'][0]) * 0.66], 
                                 color=config['colors'][1]),
                            dict(range=[config['range'][0] + (config['range'][1] - config['range'][0]) * 0.66, 
                                       config['range'][1]], 
                                 color=config['colors'][2])
                        ],
                        threshold=dict(
                            line=dict(color="white", width=4),
                            thickness=0.75,
                            value=value
                        )
                    )
                ),
                row=row, col=col
            )
        
        # Update layout
        fig.update_layout(
            height=900,
            title=dict(
                text='Real-Time Portfolio Metrics Dashboard',
                font=dict(size=28, color='white'),
                x=0.5
            ),
            template='plotly_dark',
            paper_bgcolor='rgba(0,0,0,0)',
            font=dict(color='white'),
            margin=dict(l=50, r=50, t=100, b=50)
        )
        
        performance_monitor.end_operation('realtime_metrics_dashboard')
        return fig
    
    def _create_empty_figure(self, title: str) -> go.Figure:
        """Create empty figure with error message."""
        fig = go.Figure()
        fig.update_layout(
            height=400,
            title=dict(
                text=f"{title} (Data Unavailable)",
                font=dict(size=20, color='white')
            ),
            template='plotly_dark',
            paper_bgcolor='rgba(0,0,0,0)',
            plot_bgcolor='rgba(0,0,0,0)',
            font=dict(color='white'),
            xaxis=dict(visible=False),
            yaxis=dict(visible=False),
            annotations=[dict(
                text="Data not available for this visualization",
                xref="paper",
                yref="paper",
                x=0.5,
                y=0.5,
                showarrow=False,
                font=dict(size=16, color='white')
            )]
        )
        return fig

# Initialize visualization engine
viz_engine = AdvancedVisualizationEngine()

# ============================================================================
# 4. ADVANCED RISK ANALYTICS ENGINE WITH VaR/CVaR/ES CALCULATIONS
# ============================================================================

class AdvancedRiskAnalytics:
    """Advanced risk analytics engine with comprehensive VaR/CVaR/ES calculations."""
    
    def __init__(self):
        self.methods = ['Historical', 'Parametric', 'MonteCarlo', 'EVT', 'GARCH']
        self.confidence_levels = [0.90, 0.95, 0.99, 0.995]
    
    def calculate_comprehensive_var_analysis(self, returns: pd.Series, 
                                           portfolio_value: float = 1_000_000) -> Dict:
        """Calculate comprehensive VaR analysis with all methods."""
        performance_monitor.start_operation('comprehensive_var_analysis')
        
        results = {
            'methods': {},
            'portfolio_value': portfolio_value,
            'summary': {},
            'violations': {},
            'backtest': {},
            'stress_tests': {}
        }
        
        try:
            # Calculate VaR using all methods
            for method in self.methods:
                results['methods'][method] = self._calculate_var_method(
                    returns, method, self.confidence_levels, portfolio_value
                )
            
            # Calculate summary statistics
            results['summary'] = self._calculate_var_summary(results['methods'], returns)
            
            # Calculate VaR violations
            results['violations'] = self._calculate_var_violations(
                returns, results['methods']['Historical']
            )
            
            # Perform backtesting
            results['backtest'] = self._perform_var_backtesting(
                returns, results['methods']['Historical']
            )
            
            # Perform stress tests
            results['stress_tests'] = self._perform_stress_tests(returns)
            
            # Calculate additional risk metrics
            results['additional_metrics'] = self._calculate_additional_risk_metrics(returns)
            
            performance_monitor.end_operation('comprehensive_var_analysis')
            return results
            
        except Exception as e:
            performance_monitor.end_operation('comprehensive_var_analysis', {'error': str(e)})
            error_analyzer.analyze_error_with_context(e, {'operation': 'comprehensive_var_analysis'})
            return self._create_fallback_results(returns, portfolio_value)
    
    def _calculate_var_method(self, returns: pd.Series, method: str, 
                            confidence_levels: List[float], 
                            portfolio_value: float) -> Dict:
        """Calculate VaR using specific method."""
        results = {}
        
        for confidence in confidence_levels:
            alpha = 1 - confidence
            
            if method == 'Historical':
                # Historical simulation
                var = -np.percentile(returns, alpha * 100)
                cvar = -returns[returns <= -var].mean()
                es = cvar  # For historical, ES = CVaR
                
            elif method == 'Parametric':
                # Parametric (normal distribution)
                mean = returns.mean()
                std = returns.std()
                var = -(mean + std * norm.ppf(confidence))
                cvar = -(mean - std * norm.pdf(norm.ppf(alpha)) / alpha)
                es = cvar
                
            elif method == 'MonteCarlo':
                # Monte Carlo simulation
                np.random.seed(42)
                n_simulations = 10000
                simulated_returns = np.random.normal(returns.mean(), returns.std(), n_simulations)
                var = -np.percentile(simulated_returns, alpha * 100)
                cvar = -simulated_returns[simulated_returns <= -var].mean()
                es = cvar
                
            elif method == 'EVT':
                # Extreme Value Theory
                try:
                    threshold = np.percentile(returns, 10)
                    excess = returns[returns < threshold] - threshold
                    
                    if len(excess) > 10:
                        # Generalized Pareto Distribution parameters
                        xi, beta = self._estimate_gpd_parameters(excess)
                        
                        # Calculate VaR using GPD
                        var_evt = threshold + (beta/xi) * (((len(returns) * alpha) / len(excess)) ** (-xi) - 1)
                        var = -var_evt
                        
                        # Calculate ES using GPD
                        es_evt = (var_evt + beta - xi * threshold) / (1 - xi)
                        cvar = -es_evt
                        es = cvar
                    else:
                        # Fallback to historical
                        var = -np.percentile(returns, alpha * 100)
                        cvar = -returns[returns <= -var].mean()
                        es = cvar
                        
                except:
                    var = -np.percentile(returns, alpha * 100)
                    cvar = -returns[returns <= -var].mean()
                    es = cvar
                    
            elif method == 'GARCH':
                # GARCH model VaR
                try:
                    if LIBRARY_STATUS['status'].get('arch', False):
                        from arch import arch_model
                        
                        # Fit GARCH(1,1) model
                        am = arch_model(returns * 100, vol='Garch', p=1, q=1)
                        res = am.fit(disp='off')
                        
                        # Forecast
                        forecast = res.forecast(horizon=1)
                        conditional_vol = np.sqrt(forecast.variance.values[-1, 0]) / 100
                        
                        var = -(returns.mean() + conditional_vol * norm.ppf(confidence))
                        cvar = -(returns.mean() - conditional_vol * norm.pdf(norm.ppf(alpha)) / alpha)
                        es = cvar
                    else:
                        raise ImportError("ARCH library not available")
                        
                except Exception as e:
                    # Fallback to parametric
                    mean = returns.mean()
                    std = returns.std()
                    var = -(mean + std * norm.ppf(confidence))
                    cvar = -(mean - std * norm.pdf(norm.ppf(alpha)) / alpha)
                    es = cvar
            
            else:
                # Default to historical
                var = -np.percentile(returns, alpha * 100)
                cvar = -returns[returns <= -var].mean()
                es = cvar
            
            # Store results
            results[confidence] = {
                'VaR': var,
                'VaR_absolute': var * portfolio_value,
                'CVaR': cvar,
                'CVaR_absolute': cvar * portfolio_value,
                'ES': es,
                'ES_absolute': es * portfolio_value,
                'confidence': confidence,
                'method': method
            }
        
        return results
    
    def _estimate_gpd_parameters(self, excess: pd.Series) -> Tuple[float, float]:
        """Estimate Generalized Pareto Distribution parameters."""
        # Method of moments estimation
        mean_excess = excess.mean()
        var_excess = excess.var()
        
        if var_excess > 0:
            xi = 0.5 * ((mean_excess ** 2) / var_excess + 1)
            beta = 0.5 * mean_excess * ((mean_excess ** 2) / var_excess + 1)
            return xi, beta
        else:
            return 0.1, mean_excess
    
    def _calculate_var_summary(self, methods_results: Dict, returns: pd.Series) -> Dict:
        """Calculate summary statistics for VaR analysis."""
        summary = {
            'best_method': None,
            'worst_case_var': float('inf'),
            'average_var': 0,
            'var_consistency': 0,
            'risk_adjustment_factors': {}
        }
        
        # Calculate average VaR across methods
        var_values = []
        for method, results in methods_results.items():
            for confidence, metrics in results.items():
                var_values.append(metrics['VaR'])
        
        if var_values:
            summary['average_var'] = np.mean(var_values)
            summary['worst_case_var'] = np.max(var_values)
            summary['var_consistency'] = np.std(var_values) / np.mean(var_values) if np.mean(var_values) != 0 else 0
        
        # Determine best method (lowest average VaR with consistency)
        method_scores = {}
        for method, results in methods_results.items():
            method_vars = [metrics['VaR'] for metrics in results.values()]
            avg_var = np.mean(method_vars)
            consistency = np.std(method_vars) / avg_var if avg_var != 0 else float('inf')
            
            # Score = average VaR * (1 + consistency penalty)
            method_scores[method] = avg_var * (1 + consistency * 0.5)
        
        if method_scores:
            summary['best_method'] = min(method_scores, key=method_scores.get)
        
        # Calculate risk adjustment factors
        summary['risk_adjustment_factors'] = {
            'liquidity_adjustment': self._calculate_liquidity_adjustment(returns),
            'concentration_adjustment': self._calculate_concentration_adjustment(returns),
            'tail_risk_adjustment': self._calculate_tail_risk_adjustment(returns)
        }
        
        return summary
    
    def _calculate_liquidity_adjustment(self, returns: pd.Series) -> float:
        """Calculate liquidity risk adjustment factor."""
        # Simplified liquidity adjustment based on return characteristics
        volume_factor = 1.0
        
        # Adjust based on volatility (higher volatility = lower liquidity)
        volatility = returns.std()
        if volatility > 0.02:  # More than 2% daily volatility
            volume_factor *= 1.2
        elif volatility < 0.005:  # Less than 0.5% daily volatility
            volume_factor *= 0.8
        
        # Adjust based on kurtosis (fat tails = liquidity risk)
        kurt = returns.kurtosis()
        if kurt > 3:  # Leptokurtic (fat tails)
            volume_factor *= 1.1 + min(0.5, (kurt - 3) / 10)
        
        return volume_factor
    
    def _calculate_concentration_adjustment(self, returns: pd.Series) -> float:
        """Calculate concentration risk adjustment factor."""
        # For single asset returns, concentration is high
        return 1.3  # 30% adjustment for concentration risk
    
    def _calculate_tail_risk_adjustment(self, returns: pd.Series) -> float:
        """Calculate tail risk adjustment factor."""
        # Calculate expected shortfall ratio
        var_95 = -np.percentile(returns, 5)
        cvar_95 = -returns[returns <= -var_95].mean()
        
        if var_95 != 0:
            tail_ratio = cvar_95 / var_95
            # Higher tail ratio indicates heavier tails
            return 1.0 + max(0, (tail_ratio - 1.5) * 0.2)
        
        return 1.0
    
    def _calculate_var_violations(self, returns: pd.Series, 
                                 historical_results: Dict) -> Dict:
        """Calculate VaR violations and exceptions."""
        violations = {
            'total_days': len(returns),
            'violations_95': 0,
            'violations_99': 0,
            'exception_rates': {},
            'unconditional_coverage': {},
            'conditional_coverage': {},
            'independence_test': {}
        }
        
        # Calculate violations for each confidence level
        for confidence, metrics in historical_results.items():
            var_threshold = -metrics['VaR']
            violations_count = (returns < -var_threshold).sum()
            expected_violations = len(returns) * (1 - confidence)
            
            violations[f'violations_{int(confidence*100)}'] = violations_count
            violations['exception_rates'][confidence] = {
                'actual': violations_count / len(returns),
                'expected': 1 - confidence,
                'difference': (violations_count / len(returns)) - (1 - confidence)
            }
        
        # Perform Kupiec's Proportion of Failures test (unconditional coverage)
        for confidence in [0.95, 0.99]:
            if confidence in historical_results:
                var_threshold = -historical_results[confidence]['VaR']
                violations_count = (returns < -var_threshold).sum()
                n = len(returns)
                p = 1 - confidence
                
                # Likelihood ratio test
                LR_uc = -2 * np.log(
                    ((1 - p) ** (n - violations_count) * p ** violations_count) /
                    ((1 - violations_count/n) ** (n - violations_count) * 
                     (violations_count/n) ** violations_count)
                )
                
                violations['unconditional_coverage'][confidence] = {
                    'LR_statistic': LR_uc,
                    'p_value': 1 - stats.chi2.cdf(LR_uc, 1),
                    'pass': LR_uc < 3.841  # 95% confidence chi2 critical value
                }
        
        return violations
    
    def _perform_var_backtesting(self, returns: pd.Series, 
                                historical_results: Dict) -> Dict:
        """Perform comprehensive VaR backtesting."""
        backtest = {
            'rolling_var': {},
            'conditional_coverage': {},
            'violation_clustering': {},
            'duration_between_failures': {}
        }
        
        # Calculate rolling VaR
        windows = [63, 126, 252]  # 3, 6, 12 months
        for window in windows:
            rolling_var = returns.rolling(window=window).apply(
                lambda x: -np.percentile(x, 5), raw=True
            )
            backtest['rolling_var'][window] = {
                'mean': rolling_var.mean(),
                'std': rolling_var.std(),
                'max': rolling_var.max(),
                'min': rolling_var.min()
            }
        
        # Check for violation clustering (Christoffersen's independence test)
        if 0.95 in historical_results:
            var_threshold = -historical_results[0.95]['VaR']
            violations = (returns < -var_threshold).astype(int)
            
            # Calculate transition probabilities
            n00 = n01 = n10 = n11 = 0
            for i in range(1, len(violations)):
                if violations.iloc[i-1] == 0 and violations.iloc[i] == 0:
                    n00 += 1
                elif violations.iloc[i-1] == 0 and violations.iloc[i] == 1:
                    n01 += 1
                elif violations.iloc[i-1] == 1 and violations.iloc[i] == 0:
                    n10 += 1
                elif violations.iloc[i-1] == 1 and violations.iloc[i] == 1:
                    n11 += 1
            
            # Calculate test statistic
            if (n00 + n01) > 0 and (n10 + n11) > 0:
                pi0 = n01 / (n00 + n01) if (n00 + n01) > 0 else 0
                pi1 = n11 / (n10 + n11) if (n10 + n11) > 0 else 0
                pi = (n01 + n11) / (n00 + n01 + n10 + n11)
                
                likelihood_ratio = -2 * np.log(
                    ((1 - pi) ** (n00 + n10) * pi ** (n01 + n11)) /
                    ((1 - pi0) ** n00 * pi0 ** n01 * (1 - pi1) ** n10 * pi1 ** n11)
                )
                
                backtest['conditional_coverage'] = {
                    'LR_statistic': likelihood_ratio,
                    'p_value': 1 - stats.chi2.cdf(likelihood_ratio, 1),
                    'pass': likelihood_ratio < 3.841
                }
        
        # Calculate duration between failures
        violations_idx = np.where(returns < -historical_results[0.95]['VaR'])[0]
        if len(violations_idx) > 1:
            durations = np.diff(violations_idx)
            backtest['duration_between_failures'] = {
                'mean': durations.mean(),
                'std': durations.std(),
                'min': durations.min(),
                'max': durations.max()
            }
        
        return backtest
    
    def _perform_stress_tests(self, returns: pd.Series) -> Dict:
        """Perform stress testing scenarios."""
        stress_tests = {
            'historical_scenarios': {},
            'hypothetical_scenarios': {},
            'reverse_stress_tests': {}
        }
        
        # Historical scenarios
        historical_periods = {
            '2008 Crisis': ('2008-01-01', '2009-12-31'),
            'COVID Crash': ('2020-02-01', '2020-04-30'),
            '2022 Inflation': ('2022-01-01', '2022-12-31')
        }
        
        for scenario, (start, end) in historical_periods.items():
            if isinstance(returns.index[0], pd.Timestamp):
                mask = (returns.index >= pd.Timestamp(start)) & (returns.index <= pd.Timestamp(end))
                if mask.any():
                    period_returns = returns[mask]
                    if len(period_returns) > 10:
                        stress_tests['historical_scenarios'][scenario] = {
                            'returns': period_returns.mean() * 252,
                            'volatility': period_returns.std() * np.sqrt(252),
                            'max_drawdown': self._calculate_max_drawdown(period_returns),
                            'var_95': -np.percentile(period_returns, 5),
                            'cvar_95': -period_returns[period_returns <= -np.percentile(period_returns, 5)].mean()
                        }
        
        # Hypothetical scenarios
        hypothetical_scenarios = {
            'Market Crash (-20%)': {'return_shock': -0.20, 'vol_multiplier': 2.0},
            'Volatility Spike': {'return_shock': -0.10, 'vol_multiplier': 3.0},
            'Slow Decline': {'return_shock': -0.30, 'vol_multiplier': 1.5}
        }
        
        for scenario, params in hypothetical_scenarios.items():
            stress_tests['hypothetical_scenarios'][scenario] = {
                'stressed_return': returns.mean() * 252 + params['return_shock'],
                'stressed_volatility': returns.std() * np.sqrt(252) * params['vol_multiplier'],
                'stressed_var_95': -(returns.mean() + returns.std() * params['vol_multiplier'] * norm.ppf(0.95)),
                'description': f"Return shock: {params['return_shock']:.1%}, Volatility multiplier: {params['vol_multiplier']}x"
            }
        
        # Reverse stress tests
        # What level of shock would cause a specific loss?
        target_losses = [0.10, 0.20, 0.30]  # 10%, 20%, 30% losses
        for loss in target_losses:
            # Calculate required return shock for given loss probability
            required_shock = norm.ppf(0.01) * returns.std()  # 99% confidence
            stress_tests['reverse_stress_tests'][f'{loss:.0%}_Loss'] = {
                'required_shock': required_shock,
                'probability': norm.cdf(-loss / returns.std()) if returns.std() > 0 else 0,
                'daily_return_required': -loss
            }
        
        return stress_tests
    
    def _calculate_additional_risk_metrics(self, returns: pd.Series) -> Dict:
        """Calculate additional risk metrics."""
        metrics = {
            'expected_shortfall_ratio': 0,
            'tail_risk_measures': {},
            'liquidity_risk': {},
            'concentration_risk': {}
        }
        
        # Expected Shortfall Ratio (CVaR / VaR)
        var_95 = -np.percentile(returns, 5)
        cvar_95 = -returns[returns <= -var_95].mean()
        
        if var_95 != 0:
            metrics['expected_shortfall_ratio'] = cvar_95 / var_95
        
        # Tail risk measures
        metrics['tail_risk_measures'] = {
            'skewness': returns.skew(),
            'excess_kurtosis': returns.kurtosis(),
            'tail_index': self._calculate_tail_index(returns),
            'extreme_value_index': self._calculate_extreme_value_index(returns)
        }
        
        # Liquidity risk (simplified)
        metrics['liquidity_risk'] = {
            'illiquidity_ratio': self._calculate_illiquidity_ratio(returns),
            'volume_volatility_ratio': returns.std() / (abs(returns).mean() if abs(returns).mean() > 0 else 1)
        }
        
        return metrics
    
    def _calculate_tail_index(self, returns: pd.Series) -> float:
        """Calculate tail index (Hill estimator)."""
        sorted_returns = np.sort(returns.values)
        k = max(10, len(sorted_returns) // 20)  # Use top 5% for tail estimation
        
        if k > 1:
            tail_returns = sorted_returns[:k]  # Negative returns (losses)
            hill_estimator = 1 / (np.mean(np.log(-tail_returns / (-tail_returns[-1]))) if tail_returns[-1] < 0 else 1)
            return hill_estimator
        
        return 0
    
    def _calculate_extreme_value_index(self, returns: pd.Series) -> float:
        """Calculate extreme value index."""
        # Simplified version
        excess = returns[returns < returns.quantile(0.10)] - returns.quantile(0.10)
        if len(excess) > 0 and excess.mean() != 0:
            return (excess.var() / (excess.mean() ** 2) - 1) / 2
        return 0
    
    def _calculate_illiquidity_ratio(self, returns: pd.Series) -> float:
        """Calculate illiquidity ratio (Amihud measure approximation)."""
        # Simplified: ratio of absolute return to trading range
        if len(returns) > 0:
            return abs(returns).mean() / (returns.max() - returns.min() if returns.max() != returns.min() else 1)
        return 0
    
    def _calculate_max_drawdown(self, returns: pd.Series) -> float:
        """Calculate maximum drawdown."""
        cumulative = (1 + returns).cumprod()
        rolling_max = cumulative.expanding().max()
        drawdown = (cumulative - rolling_max) / rolling_max
        return drawdown.min()
    
    def _create_fallback_results(self, returns: pd.Series, 
                                portfolio_value: float) -> Dict:
        """Create fallback results when comprehensive analysis fails."""
        return {
            'methods': {
                'Historical': {
                    0.95: {
                        'VaR': -np.percentile(returns, 5),
                        'VaR_absolute': -np.percentile(returns, 5) * portfolio_value,
                        'CVaR': -returns[returns <= np.percentile(returns, 5)].mean(),
                        'CVaR_absolute': -returns[returns <= np.percentile(returns, 5)].mean() * portfolio_value,
                        'ES': -returns[returns <= np.percentile(returns, 5)].mean(),
                        'ES_absolute': -returns[returns <= np.percentile(returns, 5)].mean() * portfolio_value,
                        'confidence': 0.95,
                        'method': 'Historical'
                    }
                }
            },
            'portfolio_value': portfolio_value,
            'summary': {
                'best_method': 'Historical',
                'worst_case_var': -np.percentile(returns, 5),
                'average_var': -np.percentile(returns, 5),
                'var_consistency': 0
            },
            'violations': {
                'total_days': len(returns),
                'violations_95': (returns < np.percentile(returns, 5)).sum(),
                'exception_rates': {
                    0.95: {
                        'actual': (returns < np.percentile(returns, 5)).sum() / len(returns),
                        'expected': 0.05,
                        'difference': (returns < np.percentile(returns, 5)).sum() / len(returns) - 0.05
                    }
                }
            }
        }

# Initialize advanced risk analytics
risk_analytics = AdvancedRiskAnalytics()

# ============================================================================
# 5. ENHANCED PORTFOLIO OPTIMIZATION ENGINE
# ============================================================================

class AdvancedPortfolioOptimizer:
    """Advanced portfolio optimization with multiple algorithms and constraints."""
    
    def __init__(self):
        self.optimization_methods = {
            'MAX_SHARPE': self._optimize_max_sharpe,
            'MIN_VARIANCE': self._optimize_min_variance,
            'MAX_RETURN': self._optimize_max_return,
            'RISK_PARITY': self._optimize_risk_parity,
            'MAX_DIVERSIFICATION': self._optimize_max_diversification,
            'HRP': self._optimize_hierarchical_risk_parity,
            'BLACK_LITTERMAN': self._optimize_black_litterman,
            'MEAN_CVAR': self._optimize_mean_cvar,
            'MEAN_VARIANCE_SEMI': self._optimize_mean_variance_semi,
            'ROBUST_OPTIMIZATION': self._optimize_robust
        }
    
    def optimize_portfolio(self, returns: pd.DataFrame, 
                          method: str = 'MAX_SHARPE',
                          constraints: Dict = None,
                          risk_free_rate: float = 0.045) -> Dict:
        """Optimize portfolio using specified method."""
        performance_monitor.start_operation(f'portfolio_optimization_{method}')
        
        try:
            if method in self.optimization_methods:
                weights, metrics = self.optimization_methods[method](
                    returns, constraints, risk_free_rate
                )
            else:
                # Default to max Sharpe
                weights, metrics = self._optimize_max_sharpe(returns, constraints, risk_free_rate)
            
            # Calculate additional metrics
            additional_metrics = self._calculate_additional_metrics(returns, weights, risk_free_rate)
            metrics.update(additional_metrics)
            
            # Create comprehensive results
            results = {
                'weights': weights,
                'metrics': metrics,
                'method': method,
                'constraints': constraints,
                'risk_free_rate': risk_free_rate,
                'timestamp': datetime.now().isoformat()
            }
            
            performance_monitor.end_operation(f'portfolio_optimization_{method}')
            return results
            
        except Exception as e:
            performance_monitor.end_operation(f'portfolio_optimization_{method}', {'error': str(e)})
            error_analyzer.analyze_error_with_context(e, {
                'operation': f'portfolio_optimization_{method}',
                'assets': len(returns.columns)
            })
            
            # Fallback to equal weight
            return self._fallback_equal_weight(returns, method, risk_free_rate)
    
    def _optimize_max_sharpe(self, returns: pd.DataFrame, 
                            constraints: Dict, risk_free_rate: float) -> Tuple[Dict, Dict]:
        """Maximize Sharpe ratio."""
        try:
            if LIBRARY_STATUS['status'].get('pypfopt', False):
                mu = expected_returns.mean_historical_return(returns)
                S = risk_models.sample_cov(returns)
                
                ef = EfficientFrontier(mu, S)
                
                # Apply constraints
                if constraints:
                    if 'bounds' in constraints:
                        ef.bounds = constraints['bounds']
                    if 'weight_bounds' in constraints:
                        ef.weight_bounds = constraints['weight_bounds']
                
                weights = ef.max_sharpe(risk_free_rate=risk_free_rate)
                cleaned_weights = ef.clean_weights()
                perf = ef.portfolio_performance(risk_free_rate=risk_free_rate)
                
                return cleaned_weights, {
                    'expected_return': perf[0],
                    'expected_volatility': perf[1],
                    'sharpe_ratio': perf[2]
                }
        
        except Exception as e:
            pass
        
        # Fallback implementation
        return self._simplified_max_sharpe(returns, risk_free_rate)
    
    def _simplified_max_sharpe(self, returns: pd.DataFrame, 
                              risk_free_rate: float) -> Tuple[Dict, Dict]:
        """Simplified max Sharpe optimization."""
        mu = returns.mean() * 252
        S = returns.cov() * 252
        
        n_assets = len(mu)
        
        def objective(weights):
            port_return = np.dot(weights, mu)
            port_risk = np.sqrt(weights.T @ S @ weights)
            if port_risk == 0:
                return 1e10  # Penalize zero risk
            return -(port_return - risk_free_rate) / port_risk
        
        # Constraints
        bounds = [(0, 1) for _ in range(n_assets)]
        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]
        
        # Initial guess (equal weights)
        initial_weights = np.ones(n_assets) / n_assets
        
        # Optimization
        result = minimize(
            objective,
            initial_weights,
            bounds=bounds,
            constraints=constraints,
            method='SLSQP',
            options={'maxiter': 1000, 'ftol': 1e-8}
        )
        
        if result.success:
            weights = result.x
            weights = weights / weights.sum()  # Normalize
            weight_dict = dict(zip(returns.columns, weights))
            
            # Calculate metrics
            port_return = np.dot(weights, mu)
            port_risk = np.sqrt(weights.T @ S @ weights)
            sharpe = (port_return - risk_free_rate) / port_risk if port_risk > 0 else 0
            
            return weight_dict, {
                'expected_return': port_return,
                'expected_volatility': port_risk,
                'sharpe_ratio': sharpe
            }
        
        # If optimization fails, return equal weights
        equal_weight = 1.0 / n_assets
        weights = {ticker: equal_weight for ticker in returns.columns}
        
        port_return = np.mean(mu)
        port_risk = np.sqrt(np.mean(np.diag(S)) / n_assets + 
                           (n_assets - 1) / n_assets * np.mean(S.values))
        sharpe = (port_return - risk_free_rate) / port_risk if port_risk > 0 else 0
        
        return weights, {
            'expected_return': port_return,
            'expected_volatility': port_risk,
            'sharpe_ratio': sharpe
        }
    
    def _optimize_min_variance(self, returns: pd.DataFrame,
                              constraints: Dict, risk_free_rate: float) -> Tuple[Dict, Dict]:
        """Minimize portfolio variance."""
        try:
            if LIBRARY_STATUS['status'].get('pypfopt', False):
                mu = expected_returns.mean_historical_return(returns)
                S = risk_models.sample_cov(returns)
                
                ef = EfficientFrontier(mu, S)
                
                if constraints:
                    if 'bounds' in constraints:
                        ef.bounds = constraints['bounds']
                
                weights = ef.min_volatility()
                cleaned_weights = ef.clean_weights()
                perf = ef.portfolio_performance()
                
                return cleaned_weights, {
                    'expected_return': perf[0],
                    'expected_volatility': perf[1],
                    'sharpe_ratio': (perf[0] - risk_free_rate) / perf[1] if perf[1] > 0 else 0
                }
        
        except Exception as e:
            pass
        
        # Fallback implementation
        S = returns.cov() * 252
        n_assets = len(returns.columns)
        
        def objective(weights):
            return weights.T @ S @ weights
        
        bounds = [(0, 1) for _ in range(n_assets)]
        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]
        
        initial_weights = np.ones(n_assets) / n_assets
        
        result = minimize(
            objective,
            initial_weights,
            bounds=bounds,
            constraints=constraints,
            method='SLSQP'
        )
        
        if result.success:
            weights = result.x
            weights = weights / weights.sum()
            weight_dict = dict(zip(returns.columns, weights))
            
            # Calculate metrics
            mu = returns.mean() * 252
            port_return = np.dot(weights, mu)
            port_risk = np.sqrt(weights.T @ S @ weights)
            
            return weight_dict, {
                'expected_return': port_return,
                'expected_volatility': port_risk,
                'sharpe_ratio': (port_return - risk_free_rate) / port_risk if port_risk > 0 else 0
            }
        
        # Equal weight fallback
        return self._fallback_equal_weight_simple(returns, risk_free_rate)
    
    def _optimize_max_return(self, returns: pd.DataFrame,
                            constraints: Dict, risk_free_rate: float) -> Tuple[Dict, Dict]:
        """Maximize portfolio return with risk constraint."""
        try:
            if LIBRARY_STATUS['status'].get('pypfopt', False):
                mu = expected_returns.mean_historical_return(returns)
                S = risk_models.sample_cov(returns)
                
                ef = EfficientFrontier(mu, S)
                
                # Default max volatility constraint
                max_vol = constraints.get('max_volatility', 0.30) if constraints else 0.30
                ef.efficient_risk(target_volatility=max_vol)
                
                weights = ef.weights
                cleaned_weights = ef.clean_weights()
                perf = ef.portfolio_performance()
                
                return cleaned_weights, {
                    'expected_return': perf[0],
                    'expected_volatility': perf[1],
                    'sharpe_ratio': (perf[0] - risk_free_rate) / perf[1] if perf[1] > 0 else 0
                }
        
        except Exception as e:
            pass
        
        # Fallback: maximize return with simple constraint
        mu = returns.mean() * 252
        n_assets = len(mu)
        
        def objective(weights):
            return -np.dot(weights, mu)  # Minimize negative return
        
        bounds = [(0, 1) for _ in range(n_assets)]
        constraints_opt = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]
        
        # Add risk constraint if specified
        if constraints and 'max_volatility' in constraints:
            S = returns.cov() * 252
            max_vol = constraints['max_volatility']
            constraints_opt.append({
                'type': 'ineq',
                'fun': lambda w: max_vol - np.sqrt(w.T @ S @ w)
            })
        
        initial_weights = np.ones(n_assets) / n_assets
        
        result = minimize(
            objective,
            initial_weights,
            bounds=bounds,
            constraints=constraints_opt,
            method='SLSQP'
        )
        
        if result.success:
            weights = result.x
            weights = weights / weights.sum()
            weight_dict = dict(zip(returns.columns, weights))
            
            # Calculate metrics
            port_return = np.dot(weights, mu)
            S = returns.cov() * 252
            port_risk = np.sqrt(weights.T @ S @ weights)
            
            return weight_dict, {
                'expected_return': port_return,
                'expected_volatility': port_risk,
                'sharpe_ratio': (port_return - risk_free_rate) / port_risk if port_risk > 0 else 0
            }
        
        return self._fallback_equal_weight_simple(returns, risk_free_rate)
    
    def _optimize_risk_parity(self, returns: pd.DataFrame,
                             constraints: Dict, risk_free_rate: float) -> Tuple[Dict, Dict]:
        """Risk parity optimization."""
        try:
            # Simplified risk parity: inverse volatility weighting
            volatilities = returns.std() * np.sqrt(252)
            inv_vol = 1 / volatilities.replace(0, np.inf)
            weights = inv_vol / inv_vol.sum()
            weight_dict = weights.to_dict()
            
            # Calculate metrics
            mu = returns.mean() * 252
            S = returns.cov() * 252
            w_array = np.array(list(weight_dict.values()))
            
            port_return = np.dot(w_array, mu)
            port_risk = np.sqrt(w_array.T @ S @ w_array)
            
            return weight_dict, {
                'expected_return': port_return,
                'expected_volatility': port_risk,
                'sharpe_ratio': (port_return - risk_free_rate) / port_risk if port_risk > 0 else 0,
                'risk_parity_score': self._calculate_risk_parity_score(returns, weight_dict)
            }
            
        except Exception as e:
            return self._fallback_equal_weight_simple(returns, risk_free_rate)
    
    def _calculate_risk_parity_score(self, returns: pd.DataFrame, weights: Dict) -> float:
        """Calculate how close the portfolio is to perfect risk parity."""
        try:
            S = returns.cov() * 252
            w_array = np.array(list(weights.values()))
            
            # Marginal contribution to risk
            mctr = (S @ w_array) / np.sqrt(w_array.T @ S @ w_array)
            
            # Risk contribution
            rctr = w_array * mctr
            
            # Perfect risk parity would have equal risk contributions
            target_contribution = 1 / len(weights)
            score = 1 - np.std(rctr) / target_contribution if target_contribution > 0 else 0
            
            return max(0, min(1, score))
            
        except:
            return 0
    
    def _optimize_max_diversification(self, returns: pd.DataFrame,
                                     constraints: Dict, risk_free_rate: float) -> Tuple[Dict, Dict]:
        """Maximum diversification optimization."""
        try:
            # Calculate correlation matrix
            corr_matrix = returns.corr()
            
            def diversification_ratio(weights):
                w = np.array(weights)
                # Portfolio volatility
                port_vol = np.sqrt(w.T @ (returns.cov() * 252) @ w)
                if port_vol == 0:
                    return 0
                # Weighted average volatility
                weighted_vol = np.sum(w * (returns.std() * np.sqrt(252)))
                return weighted_vol / port_vol
            
            n_assets = len(returns.columns)
            initial_weights = np.ones(n_assets) / n_assets
            
            bounds = [(0, 1) for _ in range(n_assets)]
            constraints_opt = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]
            
            result = minimize(
                lambda w: -diversification_ratio(w),
                initial_weights,
                bounds=bounds,
                constraints=constraints_opt,
                method='SLSQP'
            )
            
            if result.success:
                weights = result.x
                weights = weights / weights.sum()
                weight_dict = dict(zip(returns.columns, weights))
                
                # Calculate metrics
                mu = returns.mean() * 252
                S = returns.cov() * 252
                w_array = np.array(list(weight_dict.values()))
                
                port_return = np.dot(w_array, mu)
                port_risk = np.sqrt(w_array.T @ S @ w_array)
                div_ratio = diversification_ratio(w_array)
                
                return weight_dict, {
                    'expected_return': port_return,
                    'expected_volatility': port_risk,
                    'sharpe_ratio': (port_return - risk_free_rate) / port_risk if port_risk > 0 else 0,
                    'diversification_ratio': div_ratio
                }
        
        except Exception as e:
            pass
        
        return self._fallback_equal_weight_simple(returns, risk_free_rate)
    
    def _optimize_hierarchical_risk_parity(self, returns: pd.DataFrame,
                                          constraints: Dict, risk_free_rate: float) -> Tuple[Dict, Dict]:
        """Hierarchical Risk Parity optimization."""
        try:
            if LIBRARY_STATUS['status'].get('pypfopt', False):
                hrp = HRPOpt(returns)
                weights = hrp.optimize()
                
                # Calculate metrics
                mu = returns.mean() * 252
                S = returns.cov() * 252
                w_array = np.array(list(weights.values()))
                
                port_return = np.dot(w_array, mu)
                port_risk = np.sqrt(w_array.T @ S @ w_array)
                
                return weights, {
                    'expected_return': port_return,
                    'expected_volatility': port_risk,
                    'sharpe_ratio': (port_return - risk_free_rate) / port_risk if port_risk > 0 else 0
                }
        
        except Exception as e:
            pass
        
        return self._optimize_risk_parity(returns, constraints, risk_free_rate)
    
    def _optimize_black_litterman(self, returns: pd.DataFrame,
                                 constraints: Dict, risk_free_rate: float) -> Tuple[Dict, Dict]:
        """Black-Litterman optimization."""
        try:
            if LIBRARY_STATUS['status'].get('pypfopt', False):
                # This is a simplified version
                mu = expected_returns.mean_historical_return(returns)
                S = risk_models.sample_cov(returns)
                
                # Use market cap weights as equilibrium
                market_caps = np.ones(len(returns.columns))  # Placeholder
                
                bl = BlackLittermanModel(S, pi=mu, market_caps=market_caps)
                
                # Get equilibrium returns
                eq_returns = bl.equilibrium_returns()
                
                # Optimize with equilibrium returns
                ef = EfficientFrontier(eq_returns, S)
                weights = ef.max_sharpe(risk_free_rate=risk_free_rate)
                cleaned_weights = ef.clean_weights()
                perf = ef.portfolio_performance(risk_free_rate=risk_free_rate)
                
                return cleaned_weights, {
                    'expected_return': perf[0],
                    'expected_volatility': perf[1],
                    'sharpe_ratio': perf[2]
                }
        
        except Exception as e:
            pass
        
        return self._optimize_max_sharpe(returns, constraints, risk_free_rate)
    
    def _optimize_mean_cvar(self, returns: pd.DataFrame,
                           constraints: Dict, risk_free_rate: float) -> Tuple[Dict, Dict]:
        """Mean-CVaR optimization."""
        try:
            # This is a simplified implementation
            n_assets = len(returns.columns)
            n_scenarios = 1000
            confidence = 0.95
            
            # Generate scenarios
            mu = returns.mean()
            S = returns.cov()
            scenarios = np.random.multivariate_normal(mu, S, n_scenarios)
            
            def cvar_objective(weights):
                portfolio_returns = scenarios @ weights
                var = np.percentile(portfolio_returns, (1-confidence)*100)
                cvar = portfolio_returns[portfolio_returns <= var].mean()
                return cvar
            
            def return_constraint(weights):
                portfolio_return = np.dot(weights, mu * 252)
                target_return = constraints.get('target_return', 0.10) if constraints else 0.10
                return portfolio_return - target_return
            
            bounds = [(0, 1) for _ in range(n_assets)]
            constraints_opt = [
                {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},
                {'type': 'ineq', 'fun': return_constraint}
            ]
            
            initial_weights = np.ones(n_assets) / n_assets
            
            result = minimize(
                cvar_objective,
                initial_weights,
                bounds=bounds,
                constraints=constraints_opt,
                method='SLSQP'
            )
            
            if result.success:
                weights = result.x
                weights = weights / weights.sum()
                weight_dict = dict(zip(returns.columns, weights))
                
                # Calculate metrics
                mu_annual = returns.mean() * 252
                S_annual = returns.cov() * 252
                w_array = np.array(list(weight_dict.values()))
                
                port_return = np.dot(w_array, mu_annual)
                port_risk = np.sqrt(w_array.T @ S_annual @ w_array)
                
                # Calculate CVaR
                portfolio_returns_scenarios = scenarios @ w_array
                var = np.percentile(portfolio_returns_scenarios, (1-confidence)*100)
                cvar = portfolio_returns_scenarios[portfolio_returns_scenarios <= var].mean()
                
                return weight_dict, {
                    'expected_return': port_return,
                    'expected_volatility': port_risk,
                    'sharpe_ratio': (port_return - risk_free_rate) / port_risk if port_risk > 0 else 0,
                    'cvar_95': cvar,
                    'var_95': var
                }
        
        except Exception as e:
            pass
        
        return self._optimize_min_variance(returns, constraints, risk_free_rate)
    
    def _optimize_mean_variance_semi(self, returns: pd.DataFrame,
                                    constraints: Dict, risk_free_rate: float) -> Tuple[Dict, Dict]:
        """Mean-variance with semi-variance optimization."""
        try:
            # Use semi-variance instead of variance
            mu = returns.mean() * 252
            n_assets = len(returns.columns)
            
            def semivariance(weights):
                portfolio_returns = returns @ weights
                downside_returns = portfolio_returns[portfolio_returns < portfolio_returns.mean()]
                if len(downside_returns) == 0:
                    return 0
                return np.mean((downside_returns - portfolio_returns.mean()) ** 2) * 252
            
            def objective(weights):
                port_return = np.dot(weights, mu)
                semi_var = semivariance(weights)
                semi_vol = np.sqrt(semi_var) if semi_var > 0 else 0
                if semi_vol == 0:
                    return 1e10
                return -(port_return - risk_free_rate) / semi_vol
            
            bounds = [(0, 1) for _ in range(n_assets)]
            constraints_opt = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]
            
            initial_weights = np.ones(n_assets) / n_assets
            
            result = minimize(
                objective,
                initial_weights,
                bounds=bounds,
                constraints=constraints_opt,
                method='SLSQP'
            )
            
            if result.success:
                weights = result.x
                weights = weights / weights.sum()
                weight_dict = dict(zip(returns.columns, weights))
                
                # Calculate metrics
                port_return = np.dot(weights, mu)
                semi_var = semivariance(weights)
                semi_vol = np.sqrt(semi_var) if semi_var > 0 else 0
                
                # Calculate regular volatility for comparison
                S = returns.cov() * 252
                port_risk = np.sqrt(weights.T @ S @ weights)
                
                return weight_dict, {
                    'expected_return': port_return,
                    'expected_volatility': port_risk,
                    'semi_volatility': semi_vol,
                    'sharpe_ratio': (port_return - risk_free_rate) / port_risk if port_risk > 0 else 0,
                    'sortino_ratio': (port_return - risk_free_rate) / semi_vol if semi_vol > 0 else 0
                }
        
        except Exception as e:
            pass
        
        return self._optimize_max_sharpe(returns, constraints, risk_free_rate)
    
    def _optimize_robust(self, returns: pd.DataFrame,
                        constraints: Dict, risk_free_rate: float) -> Tuple[Dict, Dict]:
        """Robust optimization with uncertainty in parameters."""
        try:
            # Simple robust optimization using bootstrapping
            n_assets = len(returns.columns)
            n_bootstrap = 100
            
            # Bootstrap expected returns and covariance
            bootstrap_returns = []
            bootstrap_covs = []
            
            for _ in range(n_bootstrap):
                sample_idx = np.random.choice(len(returns), size=len(returns), replace=True)
                sample_returns = returns.iloc[sample_idx]
                bootstrap_returns.append(sample_returns.mean() * 252)
                bootstrap_covs.append(sample_returns.cov() * 252)
            
            def worst_case_sharpe(weights):
                worst_sharpe = float('inf')
                for mu_sample, S_sample in zip(bootstrap_returns, bootstrap_covs):
                    port_return = np.dot(weights, mu_sample)
                    port_risk = np.sqrt(weights.T @ S_sample @ weights)
                    if port_risk > 0:
                        sharpe = (port_return - risk_free_rate) / port_risk
                        worst_sharpe = min(worst_sharpe, sharpe)
                    else:
                        worst_sharpe = min(worst_sharpe, -1e10)
                return -worst_sharpe  # Minimize negative worst-case Sharpe
            
            bounds = [(0, 1) for _ in range(n_assets)]
            constraints_opt = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]
            
            initial_weights = np.ones(n_assets) / n_assets
            
            result = minimize(
                worst_case_sharpe,
                initial_weights,
                bounds=bounds,
                constraints=constraints_opt,
                method='SLSQP',
                options={'maxiter': 500}
            )
            
            if result.success:
                weights = result.x
                weights = weights / weights.sum()
                weight_dict = dict(zip(returns.columns, weights))
                
                # Calculate metrics using original parameters
                mu = returns.mean() * 252
                S = returns.cov() * 252
                w_array = np.array(list(weight_dict.values()))
                
                port_return = np.dot(w_array, mu)
                port_risk = np.sqrt(w_array.T @ S @ w_array)
                
                return weight_dict, {
                    'expected_return': port_return,
                    'expected_volatility': port_risk,
                    'sharpe_ratio': (port_return - risk_free_rate) / port_risk if port_risk > 0 else 0,
                    'robustness_score': self._calculate_robustness_score(returns, weights)
                }
        
        except Exception as e:
            pass
        
        return self._optimize_max_sharpe(returns, constraints, risk_free_rate)
    
    def _calculate_robustness_score(self, returns: pd.DataFrame, weights: np.ndarray) -> float:
        """Calculate robustness score of the portfolio."""
        try:
            n_bootstrap = 50
            performances = []
            
            for _ in range(n_bootstrap):
                sample_idx = np.random.choice(len(returns), size=len(returns), replace=True)
                sample_returns = returns.iloc[sample_idx]
                
                mu_sample = sample_returns.mean() * 252
                S_sample = sample_returns.cov() * 252
                
                port_return = np.dot(weights, mu_sample)
                port_risk = np.sqrt(weights.T @ S_sample @ weights)
                
                if port_risk > 0:
                    sharpe = port_return / port_risk
                    performances.append(sharpe)
            
            if len(performances) > 0:
                # Robustness: low variance in performance across bootstrap samples
                return 1 / (1 + np.std(performances))
            
        except:
            pass
        
        return 0.5
    
    def _calculate_additional_metrics(self, returns: pd.DataFrame, 
                                     weights: Dict, risk_free_rate: float) -> Dict:
        """Calculate additional portfolio metrics."""
        metrics = {}
        
        try:
            mu = returns.mean() * 252
            S = returns.cov() * 252
            w_array = np.array(list(weights.values()))
            
            # Basic metrics
            port_return = np.dot(w_array, mu)
            port_risk = np.sqrt(w_array.T @ S @ w_array)
            
            metrics.update({
                'expected_return': port_return,
                'expected_volatility': port_risk,
                'sharpe_ratio': (port_return - risk_free_rate) / port_risk if port_risk > 0 else 0
            })
            
            # Calculate portfolio returns series
            portfolio_returns = returns.dot(w_array)
            
            # Downside risk metrics
            downside_returns = portfolio_returns[portfolio_returns < 0]
            if len(downside_returns) > 0:
                downside_vol = np.std(downside_returns) * np.sqrt(252)
                metrics['downside_volatility'] = downside_vol
                metrics['sortino_ratio'] = (port_return - risk_free_rate) / downside_vol if downside_vol > 0 else 0
            else:
                metrics['downside_volatility'] = 0
                metrics['sortino_ratio'] = float('inf')
            
            # Maximum drawdown
            cumulative = (1 + portfolio_returns).cumprod()
            rolling_max = cumulative.expanding().max()
            drawdown = (cumulative - rolling_max) / rolling_max
            metrics['max_drawdown'] = drawdown.min()
            metrics['calmar_ratio'] = port_return / abs(metrics['max_drawdown']) if metrics['max_drawdown'] < 0 else 0
            
            # Omega ratio
            threshold = risk_free_rate / 252  # Daily risk-free rate
            gains = portfolio_returns[portfolio_returns > threshold].sum()
            losses = abs(portfolio_returns[portfolio_returns < threshold]).sum()
            metrics['omega_ratio'] = gains / losses if losses > 0 else float('inf')
            
            # Diversification metrics
            individual_vols = returns.std() * np.sqrt(252)
            weighted_avg_vol = np.sum(w_array * individual_vols)
            metrics['diversification_ratio'] = weighted_avg_vol / port_risk if port_risk > 0 else 1
            
            # Concentration metrics
            metrics['herfindahl_index'] = np.sum(w_array ** 2)
            metrics['effective_n_assets'] = 1 / metrics['herfindahl_index'] if metrics['herfindahl_index'] > 0 else len(w_array)
            
            # Skewness and kurtosis
            metrics['skewness'] = portfolio_returns.skew()
            metrics['kurtosis'] = portfolio_returns.kurtosis()
            
            # Tail risk
            var_95 = -np.percentile(portfolio_returns, 5)
            cvar_95 = -portfolio_returns[portfolio_returns <= -var_95].mean()
            metrics['var_95'] = var_95
            metrics['cvar_95'] = cvar_95
            metrics['expected_shortfall_ratio'] = cvar_95 / var_95 if var_95 != 0 else 0
            
            # Turnover estimation (simplified)
            metrics['estimated_turnover'] = self._estimate_turnover(weights, returns)
            
            # Liquidity score (simplified)
            metrics['liquidity_score'] = self._calculate_liquidity_score(weights, returns)
            
        except Exception as e:
            # Fill with default values if calculation fails
            metrics.update({
                'downside_volatility': 0,
                'sortino_ratio': 0,
                'max_drawdown': 0,
                'calmar_ratio': 0,
                'omega_ratio': 1,
                'diversification_ratio': 1,
                'herfindahl_index': 1/len(weights),
                'effective_n_assets': len(weights),
                'skewness': 0,
                'kurtosis': 0,
                'var_95': 0,
                'cvar_95': 0,
                'expected_shortfall_ratio': 0,
                'estimated_turnover': 0,
                'liquidity_score': 0.5
            })
        
        return metrics
    
    def _estimate_turnover(self, weights: Dict, returns: pd.DataFrame) -> float:
        """Estimate portfolio turnover."""
        try:
            # Simplified turnover estimation
            # Assume rebalancing creates turnover proportional to weight changes
            n_assets = len(weights)
            equal_weight = 1 / n_assets
            current_weights = np.array(list(weights.values()))
            
            # Calculate distance from equal weight (proxy for turnover)
            turnover = np.sum(np.abs(current_weights - equal_weight))
            return turnover
            
        except:
            return 0.5  # Default moderate turnover
    
    def _calculate_liquidity_score(self, weights: Dict, returns: pd.DataFrame) -> float:
        """Calculate portfolio liquidity score."""
        try:
            # Simplified liquidity score
            # Based on weight concentration and asset volatility
            weights_array = np.array(list(weights.values()))
            
            # Concentration penalty
            herfindahl = np.sum(weights_array ** 2)
            concentration_penalty = herfindahl * 0.5
            
            # Volatility penalty (illiquid assets often more volatile)
            volatilities = returns.std() * np.sqrt(252)
            avg_volatility = np.mean(volatilities)
            volatility_penalty = min(0.3, avg_volatility / 0.3)
            
            # Calculate score (0 = illiquid, 1 = liquid)
            score = 1 - (concentration_penalty + volatility_penalty) / 2
            return max(0, min(1, score))
            
        except:
            return 0.5
    
    def _fallback_equal_weight(self, returns: pd.DataFrame, 
                              method: str, risk_free_rate: float) -> Dict:
        """Fallback to equal weight portfolio."""
        n_assets = len(returns.columns)
        equal_weight = 1.0 / n_assets
        weights = {ticker: equal_weight for ticker in returns.columns}
        
        # Calculate metrics
        mu = returns.mean() * 252
        S = returns.cov() * 252
        w_array = np.array([equal_weight] * n_assets)
        
        port_return = np.mean(mu)
        port_risk = np.sqrt(np.mean(np.diag(S)) / n_assets + 
                           (n_assets - 1) / n_assets * np.mean(S.values))
        
        return {
            'weights': weights,
            'metrics': {
                'expected_return': port_return,
                'expected_volatility': port_risk,
                'sharpe_ratio': (port_return - risk_free_rate) / port_risk if port_risk > 0 else 0,
                'method': f'{method} (Fallback: Equal Weight)'
            },
            'method': method,
            'constraints': None,
            'risk_free_rate': risk_free_rate,
            'timestamp': datetime.now().isoformat()
        }
    
    def _fallback_equal_weight_simple(self, returns: pd.DataFrame, 
                                     risk_free_rate: float) -> Tuple[Dict, Dict]:
        """Simple equal weight fallback."""
        n_assets = len(returns.columns)
        equal_weight = 1.0 / n_assets
        weights = {ticker: equal_weight for ticker in returns.columns}
        
        mu = returns.mean() * 252
        S = returns.cov() * 252
        w_array = np.array([equal_weight] * n_assets)
        
        port_return = np.mean(mu)
        port_risk = np.sqrt(np.mean(np.diag(S)) / n_assets + 
                           (n_assets - 1) / n_assets * np.mean(S.values))
        
        return weights, {
            'expected_return': port_return,
            'expected_volatility': port_risk,
            'sharpe_ratio': (port_return - risk_free_rate) / port_risk if port_risk > 0 else 0
        }

# Initialize advanced portfolio optimizer
portfolio_optimizer = AdvancedPortfolioOptimizer()

# ============================================================================
# 6. ADVANCED DATA MANAGEMENT AND PROCESSING ENGINE
# ============================================================================

class AdvancedDataManager:
    """Advanced data management with caching, validation, and preprocessing."""
    
    def __init__(self):
        self.cache = {}
        self.cache_ttl = 3600  # 1 hour
        self.max_workers = 10
        self.retry_attempts = 3
        self.timeout = 30
    
    @st.cache_data(ttl=3600, show_spinner=True, max_entries=50)
    def fetch_advanced_market_data(self, tickers: List[str], 
                                  start_date: datetime, 
                                  end_date: datetime,
                                  interval: str = '1d',
                                  progress_callback = None) -> Dict:
        """Fetch advanced market data with multiple features."""
        performance_monitor.start_operation('fetch_advanced_market_data')
        
        try:
            data = {
                'prices': pd.DataFrame(),
                'returns': pd.DataFrame(),
                'volumes': pd.DataFrame(),
                'dividends': pd.DataFrame(),
                'splits': pd.DataFrame(),
                'metadata': {},
                'errors': {}
            }
            
            # Download data in parallel
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_ticker = {
                    executor.submit(self._fetch_single_ticker_data, 
                                  ticker, start_date, end_date, interval): ticker
                    for ticker in tickers
                }
                
                completed = 0
                total = len(tickers)
                
                for future in concurrent.futures.as_completed(future_to_ticker):
                    ticker = future_to_ticker[future]
                    completed += 1
                    
                    if progress_callback:
                        progress_callback(completed / total, f"Fetching {ticker}...")
                    
                    try:
                        ticker_data = future.result(timeout=self.timeout)
                        
                        if ticker_data and not ticker_data['prices'].empty:
                            # Add to data structures
                            if data['prices'].empty:
                                data['prices'] = ticker_data['prices']
                                data['volumes'] = ticker_data['volumes']
                            else:
                                data['prices'] = data['prices'].merge(
                                    ticker_data['prices'], left_index=True, right_index=True, how='outer'
                                )
                                data['volumes'] = data['volumes'].merge(
                                    ticker_data['volumes'], left_index=True, right_index=True, how='outer'
                                )
                            
                            # Store metadata
                            data['metadata'][ticker] = ticker_data['metadata']
                            
                            # Store dividends and splits
                            if not ticker_data['dividends'].empty:
                                data['dividends'][ticker] = ticker_data['dividends']
                            if not ticker_data['splits'].empty:
                                data['splits'][ticker] = ticker_data['splits']
                                
                        else:
                            data['errors'][ticker] = "No data returned"
                            
                    except Exception as e:
                        data['errors'][ticker] = str(e)
            
            # Process the data
            if not data['prices'].empty:
                # Forward fill and backfill missing prices
                data['prices'] = data['prices'].ffill().bfill()
                data['volumes'] = data['volumes'].ffill().bfill()
                
                # Calculate returns
                data['returns'] = data['prices'].pct_change().dropna()
                
                # Remove assets with too many missing values
                missing_threshold = 0.1  # 10% missing
                assets_to_keep = data['returns'].columns[
                    data['returns'].isnull().mean() < missing_threshold
                ]
                
                if len(assets_to_keep) > 0:
                    data['prices'] = data['prices'][assets_to_keep]
                    data['returns'] = data['returns'][assets_to_keep]
                    data['volumes'] = data['volumes'][assets_to_keep]
                else:
                    raise ValueError("No assets with sufficient data")
            
            # Calculate additional features
            if not data['returns'].empty:
                data['additional_features'] = self._calculate_additional_features(data)
            
            performance_monitor.end_operation('fetch_advanced_market_data', {
                'tickers_fetched': len(data['prices'].columns),
                'tickers_failed': len(data['errors'])
            })
            
            return data
            
        except Exception as e:
            performance_monitor.end_operation('fetch_advanced_market_data', {'error': str(e)})
            error_analyzer.analyze_error_with_context(e, {
                'operation': 'fetch_advanced_market_data',
                'tickers': tickers,
                'date_range': f"{start_date} to {end_date}"
            })
            raise
    
    def _fetch_single_ticker_data(self, ticker: str, 
                                 start_date: datetime, 
                                 end_date: datetime,
                                 interval: str) -> Dict:
        """Fetch data for a single ticker with retry logic."""
        for attempt in range(self.retry_attempts):
            try:
                stock = yf.Ticker(ticker)
                
                # Download historical data
                hist = stock.history(
                    start=start_date,
                    end=end_date,
                    interval=interval,
                    auto_adjust=True,
                    actions=True
                )
                
                if hist.empty:
                    raise ValueError(f"No historical data for {ticker}")
                
                # Extract different data types
                prices = hist['Close'].rename(ticker)
                volumes = hist['Volume'].rename(ticker)
                dividends = stock.dividends[start_date:end_date]
                splits = stock.splits[start_date:end_date]
                
                # Get metadata
                info = stock.info
                metadata = {
                    'name': info.get('longName', ticker),
                    'sector': info.get('sector', 'Unknown'),
                    'industry': info.get('industry', 'Unknown'),
                    'market_cap': info.get('marketCap', 0),
                    'beta': info.get('beta', 1.0),
                    'pe_ratio': info.get('trailingPE', 0),
                    'dividend_yield': info.get('dividendYield', 0),
                    'currency': info.get('currency', 'USD'),
                    'country': info.get('country', 'Unknown'),
                    'exchange': info.get('exchange', 'Unknown')
                }
                
                return {
                    'prices': pd.DataFrame(prices),
                    'volumes': pd.DataFrame(volumes),
                    'dividends': dividends,
                    'splits': splits,
                    'metadata': metadata
                }
                
            except Exception as e:
                if attempt == self.retry_attempts - 1:
                    raise
                time.sleep(1)  # Wait before retry
        
        return {}
    
    def _calculate_additional_features(self, data: Dict) -> Dict:
        """Calculate additional features for analysis."""
        features = {
            'technical_indicators': {},
            'statistical_features': {},
            'risk_metrics': {},
            'liquidity_metrics': {}
        }
        
        try:
            returns = data['returns']
            prices = data['prices']
            volumes = data['volumes']
            
            # Calculate technical indicators for each asset
            for ticker in returns.columns:
                ticker_returns = returns[ticker]
                ticker_prices = prices[ticker]
                
                # Basic statistics
                features['statistical_features'][ticker] = {
                    'mean': ticker_returns.mean(),
                    'std': ticker_returns.std(),
                    'skewness': ticker_returns.skew(),
                    'kurtosis': ticker_returns.kurtosis(),
                    'sharpe_ratio': ticker_returns.mean() / ticker_returns.std() if ticker_returns.std() > 0 else 0,
                    'max_drawdown': self._calculate_max_drawdown_series(ticker_returns)
                }
                
                # Risk metrics
                features['risk_metrics'][ticker] = {
                    'var_95': -np.percentile(ticker_returns, 5),
                    'cvar_95': -ticker_returns[ticker_returns <= -np.percentile(ticker_returns, 5)].mean(),
                    'expected_shortfall': -ticker_returns[ticker_returns <= -np.percentile(ticker_returns, 5)].mean()
                }
            
            # Calculate correlation matrix
            features['correlation_matrix'] = returns.corr()
            
            # Calculate covariance matrix
            features['covariance_matrix'] = returns.cov() * 252
            
            # Calculate principal components
            if len(returns.columns) > 1:
                try:
                    from sklearn.decomposition import PCA
                    
                    # Standardize returns
                    scaler = StandardScaler()
                    returns_scaled = scaler.fit_transform(returns.fillna(0))
                    
                    # Fit PCA
                    pca = PCA(n_components=min(10, len(returns.columns)))
                    pca.fit(returns_scaled)
                    
                    features['pca'] = {
                        'explained_variance_ratio': pca.explained_variance_ratio_,
                        'components': pca.components_,
                        'cumulative_variance': np.cumsum(pca.explained_variance_ratio_)
                    }
                    
                except Exception as e:
                    features['pca_error'] = str(e)
            
            # Calculate liquidity metrics
            if not volumes.empty:
                for ticker in volumes.columns:
                    if ticker in returns.columns:
                        ticker_volume = volumes[ticker]
                        ticker_returns = returns[ticker]
                        
                        # Amihud illiquidity ratio (simplified)
                        if ticker_volume.mean() > 0:
                            illiquidity = (abs(ticker_returns) / ticker_volume).mean()
                        else:
                            illiquidity = 0
                        
                        features['liquidity_metrics'][ticker] = {
                            'avg_volume': ticker_volume.mean(),
                            'volume_volatility': ticker_volume.std(),
                            'illiquidity_ratio': illiquidity,
                            'volume_turnover': ticker_volume.mean() / prices[ticker].mean() if prices[ticker].mean() > 0 else 0
                        }
            
        except Exception as e:
            features['error'] = str(e)
        
        return features
    
    def _calculate_max_drawdown_series(self, returns: pd.Series) -> float:
        """Calculate maximum drawdown for a return series."""
        cumulative = (1 + returns).cumprod()
        rolling_max = cumulative.expanding().max()
        drawdown = (cumulative - rolling_max) / rolling_max
        return drawdown.min()
    
    def validate_portfolio_data(self, data: Dict, 
                               min_assets: int = 3,
                               min_data_points: int = 100) -> Dict:
        """Validate portfolio data for analysis."""
        validation = {
            'is_valid': False,
            'issues': [],
            'warnings': [],
            'suggestions': []
        }
        
        try:
            # Check if we have prices
            if data['prices'].empty:
                validation['issues'].append("No price data available")
                return validation
            
            # Check number of assets
            n_assets = len(data['prices'].columns)
            if n_assets < min_assets:
                validation['issues'].append(f"Only {n_assets} assets available, minimum {min_assets} required")
            
            # Check data points
            n_data_points = len(data['prices'])
            if n_data_points < min_data_points:
                validation['warnings'].append(f"Only {n_data_points} data points, recommended minimum {min_data_points}")
            
            # Check for missing values
            missing_percentage = data['prices'].isnull().mean().mean()
            if missing_percentage > 0.1:  # More than 10% missing
                validation['warnings'].append(f"High percentage of missing values: {missing_percentage:.1%}")
            
            # Check for zero or negative prices
            if (data['prices'] <= 0).any().any():
                validation['issues'].append("Some assets have zero or negative prices")
            
            # Check returns calculation
            if data['returns'].empty:
                validation['issues'].append("Cannot calculate returns")
            else:
                # Check for infinite or NaN returns
                if not np.isfinite(data['returns'].values).all():
                    validation['warnings'].append("Non-finite values in returns")
                
                # Check for zero volatility assets
                zero_vol_assets = data['returns'].std()[data['returns'].std() == 0].index.tolist()
                if zero_vol_assets:
                    validation['warnings'].append(f"Zero volatility assets: {zero_vol_assets}")
            
            # Generate suggestions
            if validation['issues']:
                validation['suggestions'].extend([
                    "Check ticker symbols for validity",
                    "Extend date range for more data",
                    "Remove assets with missing data"
                ])
            
            # Final validation
            validation['is_valid'] = len(validation['issues']) == 0
            
            if validation['is_valid']:
                validation['summary'] = {
                    'assets': n_assets,
                    'data_points': n_data_points,
                    'date_range': f"{data['prices'].index[0].date()} to {data['prices'].index[-1].date()}",
                    'missing_data': f"{missing_percentage:.1%}"
                }
            
        except Exception as e:
            validation['issues'].append(f"Validation error: {str(e)}")
        
        return validation
    
    def prepare_data_for_optimization(self, data: Dict, 
                                     remove_outliers: bool = True,
                                     fill_method: str = 'ffill') -> Dict:
        """Prepare data for portfolio optimization."""
        prepared_data = data.copy()
        
        try:
            returns = data['returns'].copy()
            
            # Remove outliers (if requested)
            if remove_outliers:
                # Use IQR method to detect outliers
                Q1 = returns.quantile(0.25)
                Q3 = returns.quantile(0.75)
                IQR = Q3 - Q1
                
                # Define outlier bounds
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                # Replace outliers with bounds
                returns = returns.clip(lower_bound, upper_bound, axis=1)
            
            # Fill any remaining NaN values
            if fill_method == 'ffill':
                returns = returns.ffill().bfill()
            elif fill_method == 'zero':
                returns = returns.fillna(0)
            elif fill_method == 'mean':
                returns = returns.fillna(returns.mean())
            
            # Ensure no NaN values remain
            if returns.isnull().any().any():
                # Remove columns with NaN values
                returns = returns.dropna(axis=1)
            
            # Update prepared data
            prepared_data['returns_clean'] = returns
            
            # Calculate clean prices from clean returns
            if not returns.empty and not data['prices'].empty:
                # Get the first price for each asset
                initial_prices = data['prices'].iloc[0][returns.columns]
                
                # Reconstruct prices from clean returns
                price_reconstruction = pd.DataFrame(index=returns.index)
                for ticker in returns.columns:
                    if ticker in initial_prices:
                        cumulative_returns = (1 + returns[ticker]).cumprod()
                        price_reconstruction[ticker] = initial_prices[ticker] * cumulative_returns
                
                prepared_data['prices_clean'] = price_reconstruction
            
            # Calculate additional statistics
            prepared_data['statistics'] = {
                'mean_returns': returns.mean(),
                'volatility': returns.std(),
                'sharpe_ratios': returns.mean() / returns.std(),
                'skewness': returns.skew(),
                'kurtosis': returns.kurtosis(),
                'correlation_matrix': returns.corr(),
                'covariance_matrix': returns.cov() * 252
            }
            
        except Exception as e:
            error_analyzer.analyze_error_with_context(e, {'operation': 'prepare_data_for_optimization'})
            # Return original data if preparation fails
            prepared_data['returns_clean'] = data['returns'].fillna(0)
            prepared_data['prices_clean'] = data['prices'].fillna(method='ffill').fillna(method='bfill')
        
        return prepared_data

# Initialize advanced data manager
data_manager = AdvancedDataManager()

# ============================================================================
# 7. SMART USER INTERFACE COMPONENTS
# ============================================================================

class SmartUIComponents:
    """Smart UI components with enhanced visualization and interactivity."""
    
    @staticmethod
    def create_smart_button(label: str, key: str, icon: str = "âš¡", 
                           tooltip: str = "", variant: str = "primary") -> bool:
        """Create smart button with enhanced visualization."""
        button_html = f"""
        <style>
            .smart-button-{key} {{
                background: linear-gradient(135deg, 
                    {SmartUIComponents._get_button_color(variant, 'start')}, 
                    {SmartUIComponents._get_button_color(variant, 'end')});
                color: white;
                border: none;
                padding: 0.7rem 1.8rem;
                border-radius: 12px;
                font-weight: 700;
                font-size: 0.95rem;
                transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
                box-shadow: 0 6px 20px rgba(0, 0, 0, 0.25);
                cursor: pointer;
                display: inline-flex;
                align-items: center;
                justify-content: center;
                gap: 10px;
                min-width: 140px;
                position: relative;
                overflow: hidden;
                border: 1px solid rgba(255, 255, 255, 0.1);
            }}
            
            .smart-button-{key}:hover {{
                transform: translateY(-3px) scale(1.05);
                box-shadow: 0 12px 30px rgba(0, 0, 0, 0.35);
                border-color: rgba(255, 255, 255, 0.3);
            }}
            
            .smart-button-{key}:active {{
                transform: translateY(-1px) scale(1.02);
            }}
            
            .smart-button-{key}::before {{
                content: '';
                position: absolute;
                top: 0;
                left: -100%;
                width: 100%;
                height: 100%;
                background: linear-gradient(90deg, 
                    transparent, 
                    rgba(255, 255, 255, 0.2), 
                    transparent);
                transition: 0.5s;
            }}
            
            .smart-button-{key}:hover::before {{
                left: 100%;
            }}
            
            .smart-button-{key}.processing {{
                background: linear-gradient(135deg, #636efa, #ab63fa);
            }}
            
            .btn-icon {{
                font-size: 1.2rem;
                filter: drop-shadow(0 2px 4px rgba(0,0,0,0.3));
            }}
            
            .btn-label {{
                white-space: nowrap;
                text-shadow: 0 1px 2px rgba(0,0,0,0.3);
            }}
            
            .btn-tooltip {{
                position: absolute;
                bottom: calc(100% + 10px);
                left: 50%;
                transform: translateX(-50%);
                background: rgba(30, 30, 30, 0.98);
                color: white;
                padding: 10px 16px;
                border-radius: 8px;
                font-size: 0.85rem;
                white-space: nowrap;
                opacity: 0;
                transition: opacity 0.3s, transform 0.3s;
                pointer-events: none;
                border: 1px solid rgba(255, 255, 255, 0.15);
                box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
                backdrop-filter: blur(10px);
                z-index: 1000;
            }}
            
            .smart-button-{key}:hover .btn-tooltip {{
                opacity: 1;
                transform: translateX(-50%) translateY(-5px);
            }}
            
            .btn-tooltip::after {{
                content: '';
                position: absolute;
                top: 100%;
                left: 50%;
                transform: translateX(-50%);
                border: 6px solid transparent;
                border-top-color: rgba(30, 30, 30, 0.98);
            }}
        </style>
        
        <button class="smart-button-{key}" id="btn-{key}">
            <span class="btn-icon">{icon}</span>
            <span class="btn-label">{label}</span>
            <div class="btn-tooltip">{tooltip}</div>
        </button>
        
        <script>
            const btn{key} = document.getElementById('btn-{key}');
            
            btn{key}.addEventListener('click', function() {{
                this.classList.add('processing');
                this.querySelector('.btn-icon').textContent = 'â³';
                this.querySelector('.btn-label').textContent = 'Processing...';
                this.disabled = true;
                
                // Trigger Streamlit button click
                const event = new Event('click', {{ bubbles: true }});
                this.dispatchEvent(event);
            }});
        </script>
        """
        
        # Create columns for centered button
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            st.markdown(button_html, unsafe_allow_html=True)
            
            # Create hidden Streamlit button
            return st.button(label, key=key, help=tooltip, disabled=False, 
                           use_container_width=False, type=variant if variant != "secondary" else "secondary")
    
    @staticmethod
    def _get_button_color(variant: str, position: str) -> str:
        """Get button gradient colors based on variant."""
        colors = {
            'primary': {'start': '#00cc96', 'end': '#636efa'},
            'secondary': {'start': '#636efa', 'end': '#ab63fa'},
            'success': {'start': '#00cc96', 'end': '#00b894'},
            'warning': {'start': '#FFA15A', 'end': '#ff9f43'},
            'danger': {'start': '#ef553b', 'end': '#ff6b6b'},
            'info': {'start': '#17a2b8', 'end': '#2d98da'}
        }
        return colors.get(variant, colors['primary'])[position]
    
    @staticmethod
    def create_metric_card(title: str, value: Any, change: Any = None, 
                          icon: str = "ðŸ“Š", theme: str = "default") -> None:
        """Create enhanced metric card."""
        theme_colors = {
            'default': {'bg': 'rgba(30, 30, 30, 0.8)', 'accent': '#00cc96'},
            'success': {'bg': 'rgba(0, 204, 150, 0.1)', 'accent': '#00cc96'},
            'warning': {'bg': 'rgba(255, 161, 90, 0.1)', 'accent': '#FFA15A'},
            'danger': {'bg': 'rgba(239, 85, 59, 0.1)', 'accent': '#ef553b'},
            'info': {'bg': 'rgba(99, 110, 250, 0.1)', 'accent': '#636efa'}
        }
        
        colors = theme_colors.get(theme, theme_colors['default'])
        
        card_html = f"""
        <style>
            .metric-card-{hash(title)} {{
                background: {colors['bg']};
                border-radius: 16px;
                padding: 1.5rem;
                border: 1px solid rgba(255, 255, 255, 0.1);
                transition: all 0.3s ease;
                height: 100%;
                position: relative;
                overflow: hidden;
            }}
            
            .metric-card-{hash(title)}:hover {{
                transform: translateY(-5px);
                box-shadow: 0 12px 40px rgba(0, 0, 0, 0.3);
                border-color: {colors['accent']};
            }}
            
            .metric-card-{hash(title)}::before {{
                content: '';
                position: absolute;
                top: 0;
                left: 0;
                width: 4px;
                height: 100%;
                background: linear-gradient(180deg, {colors['accent']}, #636efa);
                opacity: 0.8;
            }}
            
            .metric-icon {{
                font-size: 2rem;
                margin-bottom: 0.5rem;
                color: {colors['accent']};
            }}
            
            .metric-title {{
                font-size: 0.9rem;
                color: #94a3b8;
                text-transform: uppercase;
                letter-spacing: 1px;
                font-weight: 600;
                margin-bottom: 0.5rem;
            }}
            
            .metric-value {{
                font-size: 2.2rem;
                font-weight: 800;
                background: linear-gradient(135deg, {colors['accent']}, #636efa);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                background-clip: text;
                margin-bottom: 0.5rem;
                line-height: 1;
            }}
            
            .metric-change {{
                font-size: 0.85rem;
                font-weight: 600;
                padding: 4px 10px;
                border-radius: 20px;
                display: inline-block;
                background: rgba(255, 255, 255, 0.1);
                border: 1px solid rgba(255, 255, 255, 0.2);
            }}
            
            .metric-change.positive {{
                background: rgba(0, 204, 150, 0.2);
                color: #00cc96;
                border-color: rgba(0, 204, 150, 0.3);
            }}
            
            .metric-change.negative {{
                background: rgba(239, 85, 59, 0.2);
                color: #ef553b;
                border-color: rgba(239, 85, 59, 0.3);
            }}
        </style>
        
        <div class="metric-card-{hash(title)}">
            <div class="metric-icon">{icon}</div>
            <div class="metric-title">{title}</div>
            <div class="metric-value">{value}</div>
            {f'<div class="metric-change {"positive" if (isinstance(change, str) and "+" in change) or (isinstance(change, (int, float)) and change > 0) else "negative"}">{change}</div>' if change else ''}
        </div>
        """
        
        st.markdown(card_html, unsafe_allow_html=True)
    
    @staticmethod
    def create_progress_tracker(steps: List[Dict], current_step: int) -> None:
        """Create interactive progress tracker."""
        progress_html = """
        <style>
            .progress-tracker {
                display: flex;
                justify-content: space-between;
                position: relative;
                margin: 2rem 0;
            }
            
            .progress-tracker::before {
                content: '';
                position: absolute;
                top: 50%;
                left: 0;
                right: 0;
                height: 3px;
                background: rgba(255, 255, 255, 0.1);
                transform: translateY(-50%);
                z-index: 1;
            }
            
            .progress-tracker .progress-line {
                position: absolute;
                top: 50%;
                left: 0;
                height: 3px;
                background: linear-gradient(90deg, #00cc96, #636efa);
                transform: translateY(-50%);
                z-index: 2;
                transition: width 0.5s ease;
            }
            
            .progress-step {
                display: flex;
                flex-direction: column;
                align-items: center;
                position: relative;
                z-index: 3;
                width: 100px;
            }
            
            .step-circle {
                width: 40px;
                height: 40px;
                border-radius: 50%;
                display: flex;
                align-items: center;
                justify-content: center;
                font-weight: 700;
                margin-bottom: 10px;
                transition: all 0.3s ease;
                border: 3px solid rgba(255, 255, 255, 0.1);
                background: rgba(30, 30, 30, 0.8);
            }
            
            .step-circle.completed {
                background: linear-gradient(135deg, #00cc96, #636efa);
                border-color: transparent;
                color: white;
                box-shadow: 0 4px 15px rgba(0, 204, 150, 0.3);
            }
            
            .step-circle.active {
                background: rgba(30, 30, 30, 0.9);
                border-color: #00cc96;
                color: #00cc96;
                box-shadow: 0 0 0 8px rgba(0, 204, 150, 0.1);
            }
            
            .step-label {
                font-size: 0.85rem;
                color: #94a3b8;
                text-align: center;
                font-weight: 500;
                margin-top: 5px;
            }
            
            .step-label.active {
                color: white;
                font-weight: 600;
            }
        </style>
        
        <div class="progress-tracker">
            <div class="progress-line" style="width: calc((100% / {}) * {});"></div>
        """.format(len(steps) - 1, current_step)
        
        for i, step in enumerate(steps):
            status = "completed" if i < current_step else "active" if i == current_step else ""
            progress_html += f"""
            <div class="progress-step">
                <div class="step-circle {status}">
                    {i + 1}
                </div>
                <div class="step-label {status if i == current_step else ''}">
                    {step['label']}
                </div>
            </div>
            """
        
        progress_html += "</div>"
        st.markdown(progress_html, unsafe_allow_html=True)
    
    @staticmethod
    def create_interactive_slider(label: str, key: str, 
                                 min_value: float, max_value: float, 
                                 value: float, step: float = None,
                                 format: str = None, 
                                 help_text: str = "") -> float:
        """Create interactive slider with enhanced UI."""
        slider_html = f"""
        <style>
            .slider-container-{key} {{
                margin: 1.5rem 0;
            }}
            
            .slider-label {{
                display: flex;
                justify-content: space-between;
                align-items: center;
                margin-bottom: 10px;
            }}
            
            .slider-title {{
                font-weight: 600;
                color: white;
                font-size: 0.95rem;
            }}
            
            .slider-value {{
                background: rgba(0, 204, 150, 0.2);
                color: #00cc96;
                padding: 4px 12px;
                border-radius: 20px;
                font-weight: 700;
                font-size: 0.9rem;
                border: 1px solid rgba(0, 204, 150, 0.3);
            }}
            
            .slider-help {{
                color: #94a3b8;
                font-size: 0.85rem;
                margin-top: 8px;
                font-style: italic;
            }}
            
            .slider-track {{
                -webkit-appearance: none;
                width: 100%;
                height: 8px;
                border-radius: 4px;
                background: rgba(255, 255, 255, 0.1);
                outline: none;
            }}
            
            .slider-track::-webkit-slider-thumb {{
                -webkit-appearance: none;
                width: 24px;
                height: 24px;
                border-radius: 50%;
                background: linear-gradient(135deg, #00cc96, #636efa);
                cursor: pointer;
                border: 3px solid rgba(30, 30, 30, 0.8);
                box-shadow: 0 4px 12px rgba(0, 204, 150, 0.3);
                transition: all 0.2s ease;
            }}
            
            .slider-track::-webkit-slider-thumb:hover {{
                transform: scale(1.1);
                box-shadow: 0 6px 20px rgba(0, 204, 150, 0.4);
            }}
            
            .slider-fill {{
                position: absolute;
                left: 0;
                top: 0;
                height: 8px;
                border-radius: 4px;
                background: linear-gradient(90deg, #00cc96, #636efa);
                pointer-events: none;
            }}
        </style>
        
        <div class="slider-container-{key}">
            <div class="slider-label">
                <span class="slider-title">{label}</span>
                <span class="slider-value" id="value-{key}">{value}</span>
            </div>
            <div style="position: relative;">
                <div class="slider-fill" id="fill-{key}" style="width: {((value - min_value) / (max_value - min_value)) * 100}%"></div>
                <input type="range" min="{min_value}" max="{max_value}" value="{value}" 
                       step="{step if step else (max_value - min_value) / 100}" 
                       class="slider-track" id="slider-{key}"
                       oninput="document.getElementById('value-{key}').textContent = this.value + '{format if format else ''}'; 
                                document.getElementById('fill-{key}').style.width = ((this.value - {min_value}) / ({max_value} - {min_value})) * 100 + '%';">
            </div>
            {f'<div class="slider-help">{help_text}</div>' if help_text else ''}
        </div>
        
        <script>
            document.getElementById('slider-{key}').addEventListener('input', function() {{
                // Update Streamlit slider
                this.dispatchEvent(new Event('change'));
            }});
        </script>
        """
        
        st.markdown(slider_html, unsafe_allow_html=True)
        
        # Create Streamlit slider
        return st.slider(
            label,
            min_value=min_value,
            max_value=max_value,
            value=value,
            step=step,
            format=format,
            help=help_text,
            key=key,
            label_visibility="collapsed"
        )
    
    @staticmethod
    def create_toggle_switch(label: str, key: str, 
                            default: bool = False,
                            help_text: str = "") -> bool:
        """Create enhanced toggle switch."""
        toggle_html = f"""
        <style>
            .toggle-container-{key} {{
                display: flex;
                align-items: center;
                justify-content: space-between;
                margin: 1rem 0;
                padding: 1rem;
                background: rgba(30, 30, 30, 0.6);
                border-radius: 12px;
                border: 1px solid rgba(255, 255, 255, 0.1);
                transition: all 0.3s ease;
            }}
            
            .toggle-container-{key}:hover {{
                background: rgba(30, 30, 30, 0.8);
                border-color: rgba(0, 204, 150, 0.3);
            }}
            
            .toggle-label {{
                font-weight: 600;
                color: white;
                font-size: 0.95rem;
            }}
            
            .toggle-help {{
                color: #94a3b8;
                font-size: 0.85rem;
                margin-top: 4px;
            }}
            
            .toggle-switch {{
                position: relative;
                display: inline-block;
                width: 60px;
                height: 30px;
            }}
            
            .toggle-switch input {{
                opacity: 0;
                width: 0;
                height: 0;
            }}
            
            .toggle-slider {{
                position: absolute;
                cursor: pointer;
                top: 0;
                left: 0;
                right: 0;
                bottom: 0;
                background-color: rgba(255, 255, 255, 0.1);
                transition: .4s;
                border-radius: 34px;
                border: 2px solid rgba(255, 255, 255, 0.2);
            }}
            
            .toggle-slider:before {{
                position: absolute;
                content: "";
                height: 22px;
                width: 22px;
                left: 4px;
                bottom: 2px;
                background-color: white;
                transition: .4s;
                border-radius: 50%;
                box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
            }}
            
            input:checked + .toggle-slider {{
                background: linear-gradient(135deg, #00cc96, #636efa);
                border-color: transparent;
            }}
            
            input:checked + .toggle-slider:before {{
                transform: translateX(28px);
                background-color: white;
            }}
            
            input:focus + .toggle-slider {{
                box-shadow: 0 0 0 3px rgba(0, 204, 150, 0.3);
            }}
        </style>
        
        <div class="toggle-container-{key}">
            <div>
                <div class="toggle-label">{label}</div>
                {f'<div class="toggle-help">{help_text}</div>' if help_text else ''}
            </div>
            <label class="toggle-switch">
                <input type="checkbox" id="toggle-{key}" {'checked' if default else ''}>
                <span class="toggle-slider"></span>
            </label>
        </div>
        
        <script>
            document.getElementById('toggle-{key}').addEventListener('change', function() {{
                // Update Streamlit checkbox
                this.dispatchEvent(new Event('change'));
            }});
        </script>
        """
        
        st.markdown(toggle_html, unsafe_allow_html=True)
        
        # Create Streamlit checkbox
        return st.checkbox(
            label,
            value=default,
            key=key,
            help=help_text,
            label_visibility="collapsed"
        )

# Initialize smart UI components
ui = SmartUIComponents()

# ============================================================================
# 8. MAIN ENHANCED APPLICATION - FIXED VERSION
# ============================================================================

class QuantEdgeProEnhanced:
    """Enhanced QuantEdge Pro application with all advanced features."""
    
    def __init__(self):
        self.data_manager = data_manager
        self.risk_analytics = risk_analytics
        self.portfolio_optimizer = portfolio_optimizer
        self.viz_engine = viz_engine
        self.ui = ui
        
        # Initialize session state
        self._initialize_session_state()
    
    def _initialize_session_state(self):
        """Initialize session state variables."""
        if 'portfolio_data' not in st.session_state:
            st.session_state.portfolio_data = None
        if 'optimization_results' not in st.session_state:
            st.session_state.optimization_results = None
        if 'risk_analysis_results' not in st.session_state:
            st.session_state.risk_analysis_results = None
        if 'current_step' not in st.session_state:
            st.session_state.current_step = 0
        if 'analysis_complete' not in st.session_state:
            st.session_state.analysis_complete = False
    
    def render_enhanced_header(self):
        """Render enhanced application header."""
        st.markdown("""
        <div class="main-header" style="text-align: center;">
            <h1 style="color: white; font-size: 3.8rem; margin-bottom: 1rem; background: linear-gradient(135deg, #00cc96, #636efa, #ab63fa); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text;">
                âš¡ QuantEdge Pro v4.0 Enhanced
            </h1>
            <p style="color: #94a3b8; font-size: 1.3rem; margin-bottom: 0.5rem;">
                Institutional Portfolio Analytics Platform with Advanced Risk Metrics
            </p>
            <p style="color: #636efa; font-size: 1.1rem;">
                Featuring: Advanced VaR/CVaR/ES Analytics â€¢ 3D Visualizations â€¢ Smart Optimization
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        # Library status indicator
        if not LIBRARY_STATUS['all_available']:
            with st.expander("ðŸ“¦ Library Status", expanded=False):
                col1, col2 = st.columns(2)
                with col1:
                    st.warning("Some optional libraries are missing:")
                    for lib in LIBRARY_STATUS['missing']:
                        st.write(f"â€¢ {lib}")
                with col2:
                    st.info("Basic functionality available. Install missing libraries for advanced features.")
    
    def render_enhanced_sidebar(self):
        """Render enhanced sidebar with smart components."""
        with st.sidebar:
            st.markdown("""
            <div style="padding: 1.5rem; border-radius: 12px; background: rgba(30, 30, 30, 0.8); margin-bottom: 2rem;">
                <h3 style="color: #00cc96; margin-bottom: 1rem;">ðŸŽ¯ Configuration Panel</h3>
            </div>
            """, unsafe_allow_html=True)
            
            # Progress tracker
            steps = [
                {'label': 'Data Setup', 'icon': 'ðŸ“Š'},
                {'label': 'Optimization', 'icon': 'âš¡'},
                {'label': 'Risk Analysis', 'icon': 'ðŸ“ˆ'},
                {'label': 'Results', 'icon': 'ðŸ“Š'}
            ]
            self.ui.create_progress_tracker(steps, st.session_state.current_step)
            
            st.markdown("---")
            
            # Asset universe selection
            st.subheader("ðŸŒ Asset Universe")
            universe_options = {
                "US Large Cap": ["AAPL", "MSFT", "GOOGL", "AMZN", "TSLA", "NVDA", "JPM", "JNJ", "V", "WMT"],
                "Technology Focus": ["AAPL", "MSFT", "GOOGL", "NVDA", "AMD", "INTC", "QCOM", "CRM", "ADBE", "ORCL"],
                "Global Diversified": ["AAPL", "MSFT", "GOOGL", "AMZN", "JPM", "JNJ", "NSRGY", "NVO", "TSM", "HSBC"],
                "Emerging Markets": ["BABA", "TSM", "005930.KS", "ITUB", "VALE", "INFY", "HDB", "IDX", "EWZ"]
            }
            
            selected_universe = st.selectbox(
                "Select Predefined Universe",
                list(universe_options.keys()),
                help="Choose from predefined asset universes"
            )
            
            # Custom tickers
            with st.expander("âž• Add Custom Assets"):
                custom_tickers = st.text_area(
                    "Enter custom tickers (comma-separated)",
                    placeholder="e.g., AAPL, MSFT, GOOGL",
                    help="Add additional assets to the portfolio"
                )
            
            # Date range with presets
            st.subheader("ðŸ“… Time Period")
            date_preset = st.selectbox(
                "Select Period",
                ["Custom", "1 Year", "3 Years", "5 Years", "10 Years"],
                help="Select predefined time periods or choose custom"
            )
            
            end_date = datetime.now()
            if date_preset == "1 Year":
                start_date = end_date - timedelta(days=365)
            elif date_preset == "3 Years":
                start_date = end_date - timedelta(days=365*3)
            elif date_preset == "5 Years":
                start_date = end_date - timedelta(days=365*5)
            elif date_preset == "10 Years":
                start_date = end_date - timedelta(days=365*10)
            else:
                col1, col2 = st.columns(2)
                with col1:
                    start_date = st.date_input("Start Date", value=end_date - timedelta(days=365*3))
                with col2:
                    end_date = st.date_input("End Date", value=end_date)
            
            # Advanced settings
            with st.expander("âš™ï¸ Advanced Settings"):
                col1, col2 = st.columns(2)
                with col1:
                    risk_free_rate = st.slider(
                        "Risk-Free Rate",
                        min_value=0.0,
                        max_value=0.20,
                        value=0.045,
                        step=0.001,
                        format="%.1%"
                    )
                with col2:
                    transaction_cost = st.slider(
                        "Transaction Cost",
                        min_value=0,
                        max_value=100,
                        value=10,
                        step=1,
                        format="%d bps"
                    )
                
                optimization_method = st.selectbox(
                    "Optimization Method",
                    list(portfolio_optimizer.optimization_methods.keys()),
                    help="Select portfolio optimization methodology"
                )
                
                # Constraints
                st.subheader("ðŸŽ¯ Constraints")
                col1, col2 = st.columns(2)
                with col1:
                    max_weight = st.slider(
                        "Max Weight",
                        min_value=0.05,
                        max_value=1.0,
                        value=0.30,
                        step=0.05,
                        format="%.0%"
                    )
                with col2:
                    min_weight = st.slider(
                        "Min Weight",
                        min_value=0.0,
                        max_value=0.20,
                        value=0.0,
                        step=0.01,
                        format="%.0%"
                    )
            
            st.markdown("---")
            
            # Action buttons
            st.subheader("ðŸš€ Actions")
            
            col1, col2 = st.columns(2)
            with col1:
                fetch_data = self.ui.create_smart_button(
                    "Fetch Data",
                    "fetch_data",
                    icon="ðŸ“¥",
                    tooltip="Download market data",
                    variant="primary"
                )
            
            with col2:
                run_analysis = self.ui.create_smart_button(
                    "Run Analysis",
                    "run_analysis",
                    icon="âš¡",
                    tooltip="Run comprehensive analysis",
                    variant="success"
                )
            
            # Reset button
            if st.button("ðŸ”„ Reset Analysis", use_container_width=True):
                self._reset_analysis()
            
            return {
                'universe': selected_universe,
                'tickers': universe_options[selected_universe] + 
                          ([t.strip().upper() for t in custom_tickers.split(',')] if custom_tickers else []),
                'start_date': start_date,
                'end_date': end_date,
                'risk_free_rate': risk_free_rate,
                'transaction_cost': transaction_cost / 10000,
                'optimization_method': optimization_method,
                'max_weight': max_weight,
                'min_weight': min_weight,
                'fetch_data': fetch_data,
                'run_analysis': run_analysis
            }
    
    def run_data_fetch(self, config: Dict):
        """Run data fetching process."""
        try:
            with st.spinner("ðŸ“¥ Fetching market data..."):
                # Update progress
                st.session_state.current_step = 1
                
                # Fetch data
                portfolio_data = self.data_manager.fetch_advanced_market_data(
                    tickers=config['tickers'],
                    start_date=config['start_date'],
                    end_date=config['end_date']
                )
                
                # Validate data
                validation = self.data_manager.validate_portfolio_data(portfolio_data)
                
                if validation['is_valid']:
                    st.session_state.portfolio_data = portfolio_data
                    st.success(f"âœ… Data fetched successfully: {validation['summary']['assets']} assets, {validation['summary']['data_points']} data points")
                    
                    # Show data preview
                    with st.expander("ðŸ“Š Data Preview", expanded=False):
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("Assets", validation['summary']['assets'])
                            st.metric("Date Range", validation['summary']['date_range'])
                        with col2:
                            st.metric("Data Points", validation['summary']['data_points'])
                            st.metric("Missing Data", validation['summary']['missing_data'])
                    
                    return True
                else:
                    st.error("âŒ Data validation failed:")
                    for issue in validation['issues']:
                        st.write(f"â€¢ {issue}")
                    return False
                    
        except Exception as e:
            error_analysis = error_analyzer.analyze_error_with_context(e, {
                'operation': 'data_fetch',
                'tickers': config['tickers'],
                'date_range': f"{config['start_date']} to {config['end_date']}"
            })
            error_analyzer.create_advanced_error_display(error_analysis)
            return False
    
    def run_portfolio_analysis(self, config: Dict):
        """Run comprehensive portfolio analysis."""
        try:
            if st.session_state.portfolio_data is None:
                st.error("Please fetch data first")
                return False
            
            # Update progress
            st.session_state.current_step = 2
            
            # Prepare data
            with st.spinner("ðŸ”„ Preparing data for analysis..."):
                prepared_data = self.data_manager.prepare_data_for_optimization(
                    st.session_state.portfolio_data,
                    remove_outliers=True
                )
            
            # Run portfolio optimization
            with st.spinner("âš¡ Optimizing portfolio..."):
                optimization_results = self.portfolio_optimizer.optimize_portfolio(
                    returns=prepared_data['returns_clean'],
                    method=config['optimization_method'],
                    constraints={
                        'bounds': (config['min_weight'], config['max_weight'])
                    },
                    risk_free_rate=config['risk_free_rate']
                )
                
                st.session_state.optimization_results = optimization_results
            
            # Run risk analysis
            with st.spinner("ðŸ“ˆ Analyzing risk metrics..."):
                portfolio_returns = prepared_data['returns_clean'].dot(
                    np.array(list(optimization_results['weights'].values()))
                )
                
                risk_analysis_results = self.risk_analytics.calculate_comprehensive_var_analysis(
                    portfolio_returns,
                    portfolio_value=1_000_000
                )
                
                st.session_state.risk_analysis_results = risk_analysis_results
            
            # Update progress
            st.session_state.current_step = 3
            st.session_state.analysis_complete = True
            
            st.success("âœ… Analysis complete!")
            return True
            
        except Exception as e:
            error_analysis = error_analyzer.analyze_error_with_context(e, {
                'operation': 'portfolio_analysis',
                'optimization_method': config['optimization_method']
            })
            error_analyzer.create_advanced_error_display(error_analysis)
            return False
    
    def render_optimization_results(self):
        """Render optimization results."""
        if st.session_state.optimization_results is None:
            return
        
        results = st.session_state.optimization_results
        
        st.markdown('<div class="section-header">âš¡ Portfolio Optimization Results</div>', 
                   unsafe_allow_html=True)
        
        # Key metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            self.ui.create_metric_card(
                "Expected Return",
                f"{results['metrics']['expected_return']:.2%}",
                icon="ðŸ“ˆ",
                theme="success"
            )
        with col2:
            self.ui.create_metric_card(
                "Expected Volatility",
                f"{results['metrics']['expected_volatility']:.2%}",
                icon="ðŸ“‰",
                theme="warning"
            )
        with col3:
            self.ui.create_metric_card(
                "Sharpe Ratio",
                f"{results['metrics']['sharpe_ratio']:.2f}",
                icon="âš¡",
                theme="info"
            )
        with col4:
            self.ui.create_metric_card(
                "Max Drawdown",
                f"{results['metrics'].get('max_drawdown', 0):.2%}",
                icon="ðŸ“Š",
                theme="danger"
            )
        
        # Portfolio allocation
        st.subheader("ðŸŽ¯ Portfolio Allocation")
        
        col1, col2 = st.columns([2, 1])
        with col1:
            # Create sunburst chart
            if st.session_state.portfolio_data:
                fig = self.viz_engine.create_portfolio_allocation_sunburst(
                    results['weights'],
                    st.session_state.portfolio_data['metadata']
                )
                st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Display weights in table
            weights_df = pd.DataFrame(
                [(ticker, f"{weight:.2%}") 
                 for ticker, weight in results['weights'].items()],
                columns=['Asset', 'Weight']
            ).sort_values('Weight', ascending=False)
            
            st.dataframe(
                weights_df,
                use_container_width=True,
                height=400
            )
        
        # Additional metrics
        st.subheader("ðŸ“Š Additional Metrics")
        
        col1, col2, col3, col4 = st.columns(4)
        metrics_to_display = [
            ('Sortino Ratio', 'sortino_ratio'),
            ('Calmar Ratio', 'calmar_ratio'),
            ('Omega Ratio', 'omega_ratio'),
            ('Diversification', 'diversification_ratio'),
            ('Effective Assets', 'effective_n_assets'),
            ('Skewness', 'skewness'),
            ('Kurtosis', 'kurtosis'),
            ('VaR 95%', 'var_95')
        ]
        
        cols = [col1, col2, col3, col4]
        for idx, (label, key) in enumerate(metrics_to_display):
            if key in results['metrics']:
                with cols[idx % 4]:
                    value = results['metrics'][key]
                    if isinstance(value, float):
                        display_value = f"{value:.3f}" if abs(value) < 10 else f"{value:.2f}"
                    else:
                        display_value = str(value)
                    
                    st.metric(label, display_value)
    
    def render_risk_analysis_results(self):
        """Render advanced risk analysis results."""
        if st.session_state.risk_analysis_results is None:
            return
        
        results = st.session_state.risk_analysis_results
        
        st.markdown('<div class="section-header">ðŸ“ˆ Advanced Risk Analytics</div>', 
                   unsafe_allow_html=True)
        
        # Create tabs for different risk analyses
        tab1, tab2, tab3, tab4 = st.tabs([
            "ðŸ“Š VaR Dashboard",
            "ðŸ“ˆ Comparative Analysis",
            "ðŸŒªï¸ Stress Testing",
            "ðŸ” Risk Metrics"
        ])
        
        with tab1:
            self._render_var_dashboard(results)
        
        with tab2:
            self._render_comparative_analysis(results)
        
        with tab3:
            self._render_stress_testing(results)
        
        with tab4:
            self._render_risk_metrics(results)
    
    def _render_var_dashboard(self, results: Dict):
        """Render VaR analysis dashboard."""
        # Get portfolio returns from optimization results
        if st.session_state.optimization_results and st.session_state.portfolio_data:
            portfolio_returns = st.session_state.portfolio_data['returns_clean'].dot(
                np.array(list(st.session_state.optimization_results['weights'].values()))
            )
            
            # Create advanced VaR dashboard
            fig = self.viz_engine.create_advanced_var_analysis_dashboard(portfolio_returns)
            st.plotly_chart(fig, use_container_width=True)
            
            # VaR summary
            st.subheader("ðŸ“‹ VaR Summary")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric(
                    "Best Method",
                    results['summary'].get('best_method', 'N/A')
                )
            with col2:
                st.metric(
                    "Worst-Case VaR",
                    f"{results['summary'].get('worst_case_var', 0):.3%}"
                )
            with col3:
                st.metric(
                    "Average VaR",
                    f"{results['summary'].get('average_var', 0):.3%}"
                )
            with col4:
                st.metric(
                    "VaR Consistency",
                    f"{results['summary'].get('var_consistency', 0):.3f}"
                )
    
    def _render_comparative_analysis(self, results: Dict):
        """Render comparative analysis."""
        st.subheader("ðŸ“Š Method Comparison")
        
        # Create comparison table
        comparison_data = []
        for method in results['methods']:
            for confidence, metrics in results['methods'][method].items():
                comparison_data.append({
                    'Method': method,
                    'Confidence': f'{confidence:.1%}',
                    'VaR': f"{metrics['VaR']:.3%}",
                    'CVaR': f"{metrics['CVaR']:.3%}",
                    'ES': f"{metrics['ES']:.3%}",
                    'VaR (Abs)': f"${metrics['VaR_absolute']:,.0f}",
                    'CVaR (Abs)': f"${metrics['CVaR_absolute']:,.0f}"
                })
        
        df_comparison = pd.DataFrame(comparison_data)
        
        # Display with styling
        st.dataframe(
            df_comparison.style.background_gradient(subset=['VaR', 'CVaR', 'ES'], cmap='Reds'),
            use_container_width=True,
            height=400
        )
        
        # Violations analysis
        if results['violations']:
            st.subheader("ðŸš¨ VaR Violations Analysis")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric(
                    "Total Days",
                    results['violations']['total_days']
                )
            with col2:
                st.metric(
                    "Violations (95%)",
                    results['violations']['violations_95'],
                    delta=f"{results['violations']['exception_rates'][0.95]['difference']:.3%}"
                )
            with col3:
                st.metric(
                    "Violations (99%)",
                    results['violations']['violations_99'],
                    delta=f"{results['violations']['exception_rates'][0.99]['difference']:.3%}"
                )
    
    def _render_stress_testing(self, results: Dict):
        """Render stress testing results."""
        st.subheader("ðŸŒªï¸ Stress Testing Scenarios")
        
        # Historical scenarios
        if results['stress_tests']['historical_scenarios']:
            st.markdown("#### ðŸ“… Historical Stress Periods")
            
            historical_data = []
            for scenario, metrics in results['stress_tests']['historical_scenarios'].items():
                historical_data.append({
                    'Scenario': scenario,
                    'Return': f"{metrics['returns']:.2%}",
                    'Volatility': f"{metrics['volatility']:.2%}",
                    'Max Drawdown': f"{metrics['max_drawdown']:.2%}",
                    'VaR 95%': f"{metrics['var_95']:.3%}",
                    'CVaR 95%': f"{metrics['cvar_95']:.3%}"
                })
            
            st.dataframe(pd.DataFrame(historical_data), use_container_width=True)
        
        # Hypothetical scenarios
        if results['stress_tests']['hypothetical_scenarios']:
            st.markdown("#### ðŸŽ¯ Hypothetical Stress Scenarios")
            
            hypothetical_data = []
            for scenario, metrics in results['stress_tests']['hypothetical_scenarios'].items():
                hypothetical_data.append({
                    'Scenario': scenario,
                    'Stressed Return': f"{metrics['stressed_return']:.2%}",
                    'Stressed Volatility': f"{metrics['stressed_volatility']:.2%}",
                    'Stressed VaR 95%': f"{metrics['stressed_var_95']:.3%}",
                    'Description': metrics['description']
                })
            
            st.dataframe(pd.DataFrame(hypothetical_data), use_container_width=True)
    
    def _render_risk_metrics(self, results: Dict):
        """Render comprehensive risk metrics."""
        st.subheader("ðŸ“Š Comprehensive Risk Metrics")
        
        if 'additional_metrics' in results:
            metrics = results['additional_metrics']
            
            # Create metrics dashboard
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### ðŸ“ˆ Tail Risk Measures")
                tail_metrics = metrics['tail_risk_measures']
                for key, value in tail_metrics.items():
                    st.metric(
                        key.replace('_', ' ').title(),
                        f"{value:.3f}" if isinstance(value, float) else str(value)
                    )
            
            with col2:
                st.markdown("#### ðŸ’§ Liquidity Metrics")
                liquidity_metrics = metrics['liquidity_risk']
                for key, value in liquidity_metrics.items():
                    st.metric(
                        key.replace('_', ' ').title(),
                        f"{value:.4f}" if isinstance(value, float) else str(value)
                    )
            
            # Expected Shortfall Ratio
            st.markdown("#### âš ï¸ Expected Shortfall Analysis")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric(
                    "Expected Shortfall Ratio",
                    f"{metrics['expected_shortfall_ratio']:.3f}"
                )
    
    def render_advanced_visualizations(self):
        """Render advanced visualizations."""
        if not st.session_state.analysis_complete:
            return
        
        st.markdown('<div class="section-header">ðŸŽ¨ Advanced Visualizations</div>', 
                   unsafe_allow_html=True)
        
        # Create tabs for different visualizations
        tab1, tab2, tab3 = st.tabs([
            "ðŸŽ¯ 3D Efficient Frontier",
            "ðŸ“Š Interactive Heatmap",
            "ðŸ“ˆ Real-Time Dashboard"
        ])
        
        with tab1:
            self._render_3d_efficient_frontier()
        
        with tab2:
            self._render_interactive_heatmap()
        
        with tab3:
            self._render_realtime_dashboard()
    
    def _render_3d_efficient_frontier(self):
        """Render 3D efficient frontier visualization."""
        if st.session_state.portfolio_data:
            returns = st.session_state.portfolio_data['returns_clean']
            
            # Get risk-free rate from config
            risk_free_rate = 0.045  # Default
            
            fig = self.viz_engine.create_3d_efficient_frontier(returns, risk_free_rate)
            st.plotly_chart(fig, use_container_width=True)
            
            st.info("""
            **3D Efficient Frontier Interpretation:**
            - **X-axis**: Portfolio Risk (Volatility)
            - **Y-axis**: Portfolio Return
            - **Z-axis**: Sharpe Ratio
            - **Color**: Sharpe Ratio (higher = better)
            
            The efficient frontier line shows optimal portfolios that maximize return for a given level of risk.
            """)
    
    def _render_interactive_heatmap(self):
        """Render interactive correlation heatmap."""
        if st.session_state.portfolio_data:
            returns = st.session_state.portfolio_data['returns_clean']
            correlation_matrix = returns.corr()
            
            fig = self.viz_engine.create_interactive_heatmap(correlation_matrix)
            st.plotly_chart(fig, use_container_width=True)
            
            # Correlation statistics
            st.subheader("ðŸ“Š Correlation Statistics")
            
            # Calculate average correlation
            mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)
            upper_tri = correlation_matrix.where(mask)
            avg_correlation = upper_tri.stack().mean()
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Average Correlation", f"{avg_correlation:.3f}")
            with col2:
                st.metric("Min Correlation", f"{correlation_matrix.min().min():.3f}")
            with col3:
                st.metric("Max Correlation", f"{correlation_matrix.max().max():.3f}")
    
    def _render_realtime_dashboard(self):
        """Render real-time metrics dashboard."""
        if st.session_state.optimization_results:
            metrics = st.session_state.optimization_results['metrics']
            
            fig = self.viz_engine.create_real_time_metrics_dashboard(metrics)
            st.plotly_chart(fig, use_container_width=True)
    
    def _reset_analysis(self):
        """Reset the analysis."""
        st.session_state.portfolio_data = None
        st.session_state.optimization_results = None
        st.session_state.risk_analysis_results = None
        st.session_state.current_step = 0
        st.session_state.analysis_complete = False
        st.rerun()
    
    def run(self):
        """Main application runner."""
        try:
            # Render header
            self.render_enhanced_header()
            
            # Render sidebar and get configuration
            config = self.render_enhanced_sidebar()
            
            # Handle data fetching
            if config['fetch_data']:
                success = self.run_data_fetch(config)
                if success:
                    st.rerun()
            
            # Handle analysis
            if config['run_analysis'] and st.session_state.portfolio_data:
                success = self.run_portfolio_analysis(config)
                if success:
                    st.rerun()
            
            # Render results if analysis is complete
            if st.session_state.analysis_complete:
                # Create main tabs
                tab1, tab2, tab3 = st.tabs([
                    "ðŸ“Š Optimization Results",
                    "ðŸ“ˆ Risk Analytics",
                    "ðŸŽ¨ Visualizations"
                ])
                
                with tab1:
                    self.render_optimization_results()
                
                with tab2:
                    self.render_risk_analysis_results()
                
                with tab3:
                    self.render_advanced_visualizations()
            
            # Performance report
            with st.sidebar:
                if st.button("ðŸ“Š Performance Report", use_container_width=True):
                    report = performance_monitor.get_performance_report()
                    with st.expander("Performance Report", expanded=True):
                        st.json(report)
            
            # Footer
            st.markdown("---")
            st.markdown("""
            <div style="text-align: center; color: #94a3b8; font-size: 0.9rem; padding: 2rem 0;">
                <p>âš¡ <strong>QuantEdge Pro v4.0 Enhanced</strong> | Advanced Portfolio Analytics Platform</p>
                <p>ðŸŽ¯ Production-Grade Analytics â€¢ 5500+ Lines of Code â€¢ Enterprise Ready</p>
                <p>ðŸ“§ Contact: support@quantedge.pro | ðŸ“ž +1 (555) 123-4567</p>
                <p style="margin-top: 1rem; font-size: 0.8rem; color: #636efa;">
                    Â© 2024 QuantEdge Technologies. All rights reserved.
                </p>
            </div>
            """, unsafe_allow_html=True)
            
        except Exception as e:
            # Global error handling
            error_analysis = error_analyzer.analyze_error_with_context(e, {
                'operation': 'application_runtime',
                'stage': 'main_application'
            })
            
            st.error("""
            ## ðŸš¨ Application Error
            
            The application encountered an unexpected error. Please try:
            
            1. Refreshing the page
            2. Reducing the number of assets
            3. Adjusting the date range
            4. Checking your internet connection
            
            If the problem persists, please contact support.
            """)
            
            with st.expander("Technical Details", expanded=False):
                error_analyzer.create_advanced_error_display(error_analysis)

# ============================================================================
# 9. MAIN EXECUTION
# ============================================================================

def main():
    """Main entry point for the enhanced application."""
    try:
        # Set page configuration
        st.set_page_config(
            page_title="QuantEdge Pro v4.0 Enhanced",
            page_icon="âš¡",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        # Add custom CSS
        st.markdown("""
        <style>
            .stApp {
                background: linear-gradient(135deg, #0e1117 0%, #1a1d2e 100%);
            }
            .main-header {
                background: linear-gradient(135deg, rgba(26, 29, 46, 0.95), rgba(42, 42, 42, 0.95));
                padding: 2.5rem;
                border-radius: 16px;
                margin-bottom: 2.5rem;
                border-left: 6px solid #00cc96;
                box-shadow: 0 12px 40px rgba(0, 0, 0, 0.4);
            }
            .section-header {
                font-size: 2rem;
                font-weight: 800;
                color: white;
                margin: 3rem 0 2rem 0;
                padding-bottom: 1rem;
                border-bottom: 2px solid;
                border-image: linear-gradient(90deg, #00cc96, #636efa) 1;
            }
        </style>
        """, unsafe_allow_html=True)
        
        # Initialize and run application
        app = QuantEdgeProEnhanced()
        app.run()
        
    except Exception as e:
        st.error(f"Critical application error: {str(e)}")
        st.code(traceback.format_exc())

if __name__ == "__main__":
    main()
